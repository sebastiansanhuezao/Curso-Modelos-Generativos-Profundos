{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3ec1cec624ef406c8f6a8d3cb7b93d46",
        "deepnote_cell_type": "markdown",
        "id": "e4Pr6pv5ie50"
      },
      "source": [
        "**MDS7203 Modelos Generativos Profundos, Primavera 2023**\n",
        "\n",
        "# Laboratorio 2: Modelo de lenguaje auto-regresivo\n",
        "\n",
        "**Profesor**: Felipe Tobar, **Auxiliares**: Cristóbal Alcázar, Camilo Carvajal Reyes, **Ayudante**: Joaquín Barceló.\n",
        "\n",
        "**Fecha de entrega**: viernes 29 de septiembre 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "38cd31c023b047f080018f94f24927e8",
        "deepnote_cell_type": "markdown",
        "id": "N08HpQApLs6x"
      },
      "source": [
        "**Nombre: Sebastián Sanhueza**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "279c16d9f94a4b5dbe45f9f4f6ff6f07",
        "deepnote_cell_type": "markdown",
        "id": "Xjt-hcwhLs6x"
      },
      "source": [
        "**Instrucciones**: El presente notebook contiene enunciado e instrucciones para la realización del laboratorio. Usted deberá completar los códigos (en este archivo o en una copia del mismo) donde se le pida hacerlo. Usted deberá entregar el notebook con sus respuestas, cumpliendo lo siguiente:\n",
        "\n",
        "- Los comentarios en código deben ser concisos pero claros. No se evaluarán sub-preguntas donde solo exista código sin comentarios pertinentes.\n",
        "- El código debe ser ordenado y ejectuable. No se evaluarán notebooks o scripts que generen errores en su ejecución. Se aconseja resetear la kernel y corroborar la correcta execución de todas las celdas antes de ejecutar el entrenamiento de su modelo.\n",
        "- Si bien se aconseja el uso de internet y otras herramientas para asistir su trabajo, asi como discusiones con el ED y estudiantes, el código que entregue debe ser de su autoría.\n",
        "\n",
        "El objetivo del laboratorio será implementar, desde casi cero, un modelo de lenguaje estilo GPT, i.e., basado en el uso de un bloque \"decoder\" de la arquitectura Transformer (como se muestra en la imagen a continuación). Este tipo de modelos es un ejemplo de modelo auto-regresivo y que ha tenido gran relevancia en el último tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "-NgdWMHtLs6y",
        "outputId": "a4f0843d-46ef-4864-e981-9d7a736f6f07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://i.stack.imgur.com/bWnx0.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://i.stack.imgur.com/bWnx0.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzr2TNahLs6z"
      },
      "source": [
        "Algunos links útiles:\n",
        "\n",
        "* [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "* [GPT2, original blog post](https://openai.com/research/better-language-models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "40d3aa47f3204ca888f874beb273d9f5",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "vsiWqJQsLs6z"
      },
      "source": [
        "### Resumen de preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "14369c673f3049a19e647a78a00686d4",
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "m2Jcqz9nLs60"
      },
      "source": [
        "- [ ] a) (0,5 ptos.) Definición de diccionarios para vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c3ae5920aed6440ea4c012068c08bb8b",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "9sqO5_aZLs60"
      },
      "source": [
        "- [ ] b) (bonus) Utilización de embeddings previo a la normalización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d259ba9b76b74b2e8eb0132b06f7f31c",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "lMC59nHaLs60"
      },
      "source": [
        "- [ ] c) (bonus) Escalamiento por $1/\\sqrt{d_k}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "14547d4ff1564ceca8f1133b2bf3831f",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "Uory2a5QLs60"
      },
      "source": [
        "- [ ] d) (1.5 ptos.) Creación de clase `Head`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "806e04b64a5f4e8c9c46b8addb61da30",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "YAVxRGCcLs61"
      },
      "source": [
        "- [ ] e) (0.75 pto.) Implementación de clase `FeedForward`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "af2c2194e9e94407b2ed8d38ac7dd9c5",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "r8Vn53TELs61"
      },
      "source": [
        "- [ ] f) (0.5 ptos.) Relación entre hiper-parámetros n_head y head_size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9cec4a573e824ff2a535781db9773328",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "0XGUGfNwLs61"
      },
      "source": [
        "- [ ] g) (0.75 ptos.) Forward pass en `DecoderBlock`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "33851255223c4c569a4ef98add7b0b4f",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "WokI6TIILs61"
      },
      "source": [
        "- [ ] h) (1 pto.) Implementación clase `GPTLM`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "273f32755bf446cba8132463ead5c3ac",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "zdEyPdMpLs61"
      },
      "source": [
        "- [ ] i) (1 pto.) Training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkVMwOPKLs62"
      },
      "source": [
        "- [ ] j) (bonus) Comparar modelo con Baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "3381cfa7678849408eeaf9aedb7fb2d1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5134,
        "execution_start": 1693917871424,
        "id": "xIPkmAHJmC9s",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ZwlYE_Ls62"
      },
      "source": [
        "Características de la GPU, si es que está disponible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6593dVMLs62",
        "outputId": "10f8e327-1cf7-403b-9197-8311aba3dca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 30 05:13:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a607d5c593c64504a2c445b78a01b637",
        "deepnote_cell_type": "markdown",
        "id": "O54vJC-DjbMe"
      },
      "source": [
        "## 1. Corpus 📖\n",
        "\n",
        "Escoja uno de los dos datasets:\n",
        "- shakespeare.txt: concatenación de obras de shakespeare, ~ 1 millon de caracteres\n",
        "- cabromagico.txt: concatenación de los libros de Harry Potter, ~ 6 millones de caracteres\n",
        "\n",
        "Escoja en base a sus gustos y capacidades de cómputo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x0TnAbzLs63",
        "outputId": "142c124e-1859-4120-f1ea-364b435df000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-30 05:13:56--  https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/cabromagico.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6425820 (6.1M) [text/plain]\n",
            "Saving to: ‘cabromagico.txt’\n",
            "\n",
            "cabromagico.txt     100%[===================>]   6.13M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-09-30 05:13:57 (70.6 MB/s) - ‘cabromagico.txt’ saved [6425820/6425820]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Si usan collab, descargar dataset desde repo GAMES descomentando una de las siguientes lineas:\n",
        "# !wget https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/shakespeare.txt\n",
        "!wget https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/cabromagico.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc5zIGDoLs63",
        "outputId": "e37009f9-cbba-42f3-f955-1837cd91cc3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del corpus (cabromagico.txt): 6,340,988 caracteres\n"
          ]
        }
      ],
      "source": [
        "# Selecciona el corpus a su gusto\n",
        "#filename = 'shakespeare.txt'\n",
        "filename = 'cabromagico.txt'\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(f\"Tamaño del corpus ({filename}): {len(text):,} caracteres\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxRieasJLs63",
        "outputId": "14117757-80b5-4336-d07c-1f18d4fff7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            "\f\u001f !\"$%&'()*,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|}~é–—‘’“”…　【】下为书件作你做全制区坛子式志您文新最本来格电的社立米糯自要论载\n",
            "137\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "063a10ca987f4b479fb3b53b92f2ad52",
        "deepnote_cell_type": "markdown",
        "id": "Dv_9XZrpLs64"
      },
      "source": [
        "> a) (0.5 ptos.) Dada una lista de ordenada de caracteres, defina:\n",
        "> - stoi: un diccionario caracter -> índice\n",
        "> - itos: un diccionario índice -> caracter\n",
        "> Con lo anterior, defina dos funciones encode y decode que tomen un string y una lista de índices respectivamente y devuelvan una lista de índices y un string según corresponda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "599ecea77d984abaace18d2f1cc934f1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 9,
        "execution_start": 1693859157194,
        "id": "NURlEkGjkSEf",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------\n",
        "\n",
        "# Diccionario tipo caracter:índice\n",
        "stoi = {char:index for index,char in enumerate(chars)}\n",
        "\n",
        "# Diccionario tipo índice:caracter\n",
        "itos = {index:char for index,char in enumerate(chars)}\n",
        "\n",
        "# encoder: toma un string, devuelve una lista de índices\n",
        "encode = lambda s: [stoi[element] for element in s]\n",
        "\n",
        "# decoder: toma una lista de índices, devuelve un string\n",
        "decode = lambda l: ''.join([itos[element] for element in l])\n",
        "\n",
        "# --------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fveui6wELs64"
      },
      "outputs": [],
      "source": [
        "if filename == 'shakespeare.txt':\n",
        "    assert encode('hola, que tal?') == [46, 53, 50, 39, 6, 1, 55, 59, 43, 1, 58, 39, 50, 12], 'Verifica que el output entregue una lista de enteros'\n",
        "    assert decode(encode('hola, que tal?')) == 'hola, que tal?', 'Debe ser un string'\n",
        "elif filename == 'cabromagico.txt':\n",
        "    assert encode('hola, que tal?') == [73, 80, 77, 66, 14, 4, 82, 86, 70, 4, 85, 66, 77, 33], 'Verifica que el output entregue una lista de enteros'\n",
        "    assert decode(encode('hola, que tal?')) == 'hola, que tal?', 'Debe ser un string'\n",
        "else:\n",
        "    print('Estas usando un filename distinto para construir el corpus!\\nSi estas explorando otro corpus esta bien!\\nRecuerda verificar que la funcionalidad esta correcta con shakespear o cabromagico.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "488abcf1ec984a438cab8626fc6e8160",
        "deepnote_cell_type": "markdown",
        "id": "ktvBOC1qmJDY"
      },
      "source": [
        "Nuestro modelo no entiende el lenguaje directamente, sino que los representa como números. Pasamos el corpus completo a su representación de enteros, usando el `stoi` (aka _string-to-index_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cell_id": "9e79dd31677a477580450a4b80e2aa95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 136,
        "execution_start": 1693859161854,
        "id": "WAsyhejrl2X6",
        "outputId": "01a9bc18-63d4-4ae8-8660-332d83a8c5ef",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6340988])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "39c418201e2a4534a13ddc5527929041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "d9aOmlLVlEcr",
        "outputId": "27575047-af89-4f88-8377-3930f9cbf90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto con los primeros 100 caracteres:\n",
            "----------------------------------------------\n",
            "\n",
            "Harry Potter and the Sorcerer's Stone\n",
            "CHAPTER ONE\n",
            "THE BOY WHO LIVED\n",
            "Mr. and Mrs. Dursley, of number \n",
            "\n",
            "----------------------------------------------\n",
            "Su representación como tensor de PyTorch...\n",
            "\n",
            "tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70, 83,  4, 66, 79, 69,  4, 85,\n",
            "        73, 70,  4, 52, 80, 83, 68, 70, 83, 70, 83, 10, 84,  4, 52, 85, 80, 79,\n",
            "        70,  1, 36, 41, 34, 49, 53, 38, 51,  4, 48, 47, 38,  1, 53, 41, 38,  4,\n",
            "        35, 48, 58,  4, 56, 41, 48,  4, 45, 42, 55, 38, 37,  1, 46, 83, 16,  4,\n",
            "        66, 79, 69,  4, 46, 83, 84, 16,  4, 37, 86, 83, 84, 77, 70, 90, 14,  4,\n",
            "        80, 71,  4, 79, 86, 78, 67, 70, 83,  4])\n"
          ]
        }
      ],
      "source": [
        "N=100\n",
        "print(f\"Texto con los primeros {N} caracteres:\\n----------------------------------------------\\n\")\n",
        "print(text[:N])\n",
        "print(\"\\n----------------------------------------------\\nSu representación como tensor de PyTorch...\\n\")\n",
        "print(data[:N])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5aa3280c539a40c7a350543d116126c6",
        "deepnote_cell_type": "markdown",
        "id": "Aqdw5-8BnVhX"
      },
      "source": [
        "## 2. Separar el dataset 🔨 y 🎓"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "4ee3ee4d9f4c425e86da95a874ce263f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 14,
        "execution_start": 1693859215823,
        "id": "zzUjG0OqnWr4",
        "outputId": "e9bff93f-37b1-4382-c3ea-cba972d61805",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Tamaño del corpus de entrenamiento: 5,706,889 (0.90) caracteres\n",
            "--> Tamaño del corpus de validación: 634,099 (0.10) caracteres\n"
          ]
        }
      ],
      "source": [
        "n = int(0.9 * len(data))  # 90%\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"--> Tamaño del corpus de entrenamiento: {len(train_data):,} ({(train_data.shape[0] / data.shape[0]):.2f}) caracteres\")\n",
        "print(f\"--> Tamaño del corpus de validación: {len(val_data):,} ({(val_data.shape[0] / data.shape[0]):.2f}) caracteres\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "91dfd4134b1c44ba8b699996afd1cc7d",
        "deepnote_cell_type": "markdown",
        "id": "JaWNJQnzpzC8"
      },
      "source": [
        "Sobre la mécanica de datos y etiquetas,\n",
        "\n",
        "* Accedemos a los datos a partir de \"fragmentos contextuales\"; esto es un bloque de texto en representación númerica de tamaño `block_size`\n",
        "* El modelo es semi-supervisado, es decir, búscamos entrenar un modelo de tal forma que dado ${x}_{i:j}$ _tokens_, vamos a predecir el siguiente _token_ $x_{j+1}$\n",
        "* Las etiquetas emergen del mismo bloque contextual moviendo la ventana con un _offset_ de 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "97a1b7a307504727b77d318f9923413a",
        "deepnote_cell_type": "markdown",
        "id": "zFVaZzxuq0H3"
      },
      "source": [
        "Por ejemplo, dado un bloque de tamaño 8,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "b4b1cc1b8e7d4b9795c52191988b8056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 23,
        "execution_start": 1693859220379,
        "id": "ouHuc50ApdRS",
        "outputId": "8d1b5664-969a-4de1-9e81-1406f06c528f",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Una bloque contextual (X, Y) será:\n",
            "\n",
            "X: [41, 66, 83, 83, 90, 4, 49, 80, 85, 85, 70, 83, 4]\n",
            "  --> decode(X): Harry Potter \n",
            "------------------------------------\n",
            "Y: [66, 83, 83, 90, 4, 49, 80, 85, 85, 70, 83, 4, 66]\n",
            "  --> decode(Y): arry Potter a\n"
          ]
        }
      ],
      "source": [
        "block_size = 13\n",
        "print(f\"Una bloque contextual (X, Y) será:\\n\")\n",
        "print(f\"X: {[x.item() for x in data[:block_size]]}\")\n",
        "print(f\"  --> decode(X): {decode([x.item() for x in data[:block_size]])}\")\n",
        "print('------------------------------------')\n",
        "print(f\"Y: {[x.item() for x in data[1:block_size+1]]}\")\n",
        "print(f\"  --> decode(Y): {decode([y.item() for y in data[1:block_size+1]])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d2f37b21b42246a3adfb3998f07f9387",
        "deepnote_cell_type": "markdown",
        "id": "ukclPGdHwALB"
      },
      "source": [
        "Sin embago, dentro de cada bloque contextual ocupamos la información de manera autoregresiva, generando múltiple observaciones a partir de este..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "07f3c398835145b38ffa4db9f8b924e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 17,
        "execution_start": 1693859223706,
        "id": "NBir0GCgshQH",
        "outputId": "d2bdd76a-1b4c-4825-9e8f-53a6a0379e5a",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuando el input es tensor([41]) el target es: 66\n",
            "Cuando el input es tensor([41, 66]) el target es: 83\n",
            "Cuando el input es tensor([41, 66, 83]) el target es: 83\n",
            "Cuando el input es tensor([41, 66, 83, 83]) el target es: 90\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90]) el target es: 4\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4]) el target es: 49\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49]) el target es: 80\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80]) el target es: 85\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85]) el target es: 85\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85]) el target es: 70\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70]) el target es: 83\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70, 83]) el target es: 4\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70, 83,  4]) el target es: 66\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"Cuando el input es {context} el target es: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7ed1dfd8af584e109c9348cd1d0912db",
        "deepnote_cell_type": "markdown",
        "id": "AeRBbWV6z75E"
      },
      "source": [
        "Por lo tanto, cada bloque contextual, genera un número de observaciones igual a su tamaño.\n",
        "\n",
        "En términos de _batches_, podemos procesar en paralelo, múltiples bloques contextuales. Lo importante es que cada bloque contextual es independiente, y no hay computo que ocurra a nivel transversal, sino paralelo entre estos. No se mezclan las secuencias autoregresivas de cada contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "779633ce16194ccfbcc43f79bf338b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 265,
        "execution_start": 1693859227529,
        "id": "X3hWnI_5yp7x",
        "outputId": "bd0cf70c-d9e0-4e94-ed0b-cb31160a4fad",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  6,   4,   1,   6,  47,  80,  14,   6],\n",
            "        [  4,  85,  80,  80,  33,   6,   1, 103],\n",
            "        [ 71,   4,  71,  80,  86,  83,   4,  73],\n",
            "        [ 73,  66,  85,  33,   6,   1,   6,  34]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  4,   1,   6,  47,  80,  14,   6,   4],\n",
            "        [ 85,  80,  80,  33,   6,   1, 103, 103],\n",
            "        [  4,  71,  80,  86,  83,   4,  73,  80],\n",
            "        [ 66,  85,  33,   6,   1,   6,  34,   4]])\n",
            "----\n",
            "Cuando el input es [6] el target: 4\n",
            "Cuando el input es [6, 4] el target: 1\n",
            "Cuando el input es [6, 4, 1] el target: 6\n",
            "Cuando el input es [6, 4, 1, 6] el target: 47\n",
            "Cuando el input es [6, 4, 1, 6, 47] el target: 80\n",
            "Cuando el input es [6, 4, 1, 6, 47, 80] el target: 14\n",
            "Cuando el input es [6, 4, 1, 6, 47, 80, 14] el target: 6\n",
            "Cuando el input es [6, 4, 1, 6, 47, 80, 14, 6] el target: 4\n",
            "Cuando el input es [4] el target: 85\n",
            "Cuando el input es [4, 85] el target: 80\n",
            "Cuando el input es [4, 85, 80] el target: 80\n",
            "Cuando el input es [4, 85, 80, 80] el target: 33\n",
            "Cuando el input es [4, 85, 80, 80, 33] el target: 6\n",
            "Cuando el input es [4, 85, 80, 80, 33, 6] el target: 1\n",
            "Cuando el input es [4, 85, 80, 80, 33, 6, 1] el target: 103\n",
            "Cuando el input es [4, 85, 80, 80, 33, 6, 1, 103] el target: 103\n",
            "Cuando el input es [71] el target: 4\n",
            "Cuando el input es [71, 4] el target: 71\n",
            "Cuando el input es [71, 4, 71] el target: 80\n",
            "Cuando el input es [71, 4, 71, 80] el target: 86\n",
            "Cuando el input es [71, 4, 71, 80, 86] el target: 83\n",
            "Cuando el input es [71, 4, 71, 80, 86, 83] el target: 4\n",
            "Cuando el input es [71, 4, 71, 80, 86, 83, 4] el target: 73\n",
            "Cuando el input es [71, 4, 71, 80, 86, 83, 4, 73] el target: 80\n",
            "Cuando el input es [73] el target: 66\n",
            "Cuando el input es [73, 66] el target: 85\n",
            "Cuando el input es [73, 66, 85] el target: 33\n",
            "Cuando el input es [73, 66, 85, 33] el target: 6\n",
            "Cuando el input es [73, 66, 85, 33, 6] el target: 1\n",
            "Cuando el input es [73, 66, 85, 33, 6, 1] el target: 6\n",
            "Cuando el input es [73, 66, 85, 33, 6, 1, 6] el target: 34\n",
            "Cuando el input es [73, 66, 85, 33, 6, 1, 6, 34] el target: 4\n"
          ]
        }
      ],
      "source": [
        "# colocar seed como su RUT\n",
        "torch.manual_seed(206684402)\n",
        "batch_size = 4\n",
        "block_size = 8  # largo de ventana máximo para considerar en la precisión\n",
        "# Estos parámetros se re-definirán para el entrenamiento final\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"Cuando el input es {context.tolist()} el target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1889b0faf3d648409045388624948ef6",
        "deepnote_cell_type": "markdown",
        "id": "U-xAeNbk1f2E"
      },
      "source": [
        "Lo que recibirá la red como _input_ será:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "ace2e494f0af47718468b6e75eb1dd54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 17,
        "execution_start": 1693859236814,
        "id": "gTCL2aQY1VbI",
        "outputId": "56fc2689-59a4-4abb-d037-e0ae418b99b6",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  6,   4,   1,   6,  47,  80,  14,   6],\n",
              "        [  4,  85,  80,  80,  33,   6,   1, 103],\n",
              "        [ 71,   4,  71,  80,  86,  83,   4,  73],\n",
              "        [ 73,  66,  85,  33,   6,   1,   6,  34]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "xb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8f4d24230cbc46af97f97d7090c150d4",
        "deepnote_cell_type": "markdown",
        "id": "OPG_QkSJ1km7"
      },
      "source": [
        "## 3. Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6e48f01409934cf88739d426f3a1b144",
        "deepnote_cell_type": "markdown",
        "id": "wbhkQX1QLs7I"
      },
      "source": [
        "Creamos un modelo base clásico, para luego compararlo con nuestro Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "79776665f2914e0dad0b035148df8a82",
        "deepnote_cell_type": "code",
        "id": "MlAT7Lnx1hyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f9156f-21e5-43c9-9543-009420e90f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 137])\n",
            "tensor(5.6164, grad_fn=<NllLossBackward0>)\n",
            "\t全【'下d‘立”区Mt为=iag:,来Iq的?Uh1你的xB'D<>】CPjK文1ZCU你Ye全为\\糯'i(^l*\n",
            "Jh论社2您做x'}&?[^B书Z!95:[T2`您nt社d4格e下”~4HVr3?\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # cada token lee sucesivamente los logits para el token siguiente de una lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx y targets son ambos tensores de tamaño (B,T) con elementos enteros\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx es un arreglo (B, T) de indices del contexto actual\n",
        "        for _ in range(max_new_tokens):\n",
        "            # obtener predicciones\n",
        "            logits, loss = self(idx)\n",
        "            # concentrarse en el último paso\n",
        "            logits = logits[:, -1, :]  # se convierte en (B, C)\n",
        "            # aplicamos softmax para obtener probabilidades\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # samplear de la distribución\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # agregar el índice de la muestra a la secuencia actual\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d163007f6061430fa982a27a84438b23",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "ZH2FDMMqLs7J"
      },
      "source": [
        "## 4. Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0f104b88006e4923a61a73a4d2c7cf87",
        "deepnote_cell_type": "markdown",
        "id": "umjOJn2hLs7J"
      },
      "source": [
        "En la sección anterior, vimos que cada _token_ toma una representación vectorial llamada _embeddings_.\n",
        "\n",
        "Nuestro corpus contiene $65$ caracteres únicos con un _embedding_ asociado a cada _token_. La idea de representar el lenguaje en términos de tokens, y estos a su vez en vectores, es que podemos aprender estas representaciones vectoriales a partir de los datos. Sin embargo, la representación es única, y muchas veces un mismo _token_ puede tener distintos significados según su contexto. Por ejemplo:\n",
        "1. \"Te banco a morir!\"\n",
        "2. \"El banco está abierto hasta las dos.\"\n",
        "\n",
        "En ambas oraciones anteriores, el token `banco` tiene un significado distinto. Se espera entonces que la representación de ese token sea distinta en ambos casos y eso lo logramos con la influencia de los tokens presentes en el mismo contexto.\n",
        "\n",
        "La idea principal de _self-attention_ es utilizar la secuencia de _embeddings_ dentro de un contexto para computar un promedio ponderado a partir de estos. Dado una secuencia de _embeddings_ de _tokens_ $x_1, \\dots, x_n$, el mecanismo de _self-attention_ (o auto-atención) produce una nueva secuencia de _embeddings_ $x'_1, \\dots, x'_n$, donde cada $x'_i$ es una combinación lineal de todos los $x_j$:\n",
        "\n",
        "$$\n",
        "x'_i = \\sum_{j=1}^{n} \\alpha_{ij} x_{j}\n",
        "$$\n",
        "\n",
        "Los coeficientes $\\alpha_{ij}$ se llaman ponderadores de atención y están normalizados tal que $\\sum_{j}\\alpha_{ji}=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9e921ba45c064d5d9628b6dc9f470c9b",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "i4KEpbQuLs7J"
      },
      "source": [
        "En términos sencillos, construiremos un mecanismo de comunicación entre distintos tokens dentro del bloque de contexto, que se representará por una colección de ponderadores en una matriz. Esta colección de ponderadores la llamaremos matriz de atención (o self-attention) y nos permitirá vía la operación de multiplicación de matrices, agregar distintos valores dentro de un bloque contextual en una sola cantidad. Spoiler, estos pesos serán data-dependientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9296b572acca4bad82f5cfc62690af5b",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "CnzdFOQOLs7K"
      },
      "source": [
        "Comencemos emulando la operación con pesos fijos, usaremos la parte triangular inferior de una matriz identidad de 3x3, la cual normalizaremos a nivel de fila."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "b8c36015c3a24412a5f725c06a0e08ac",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 44,
        "execution_start": 1693860540618,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QeKp7JTLs7K",
        "outputId": "08aa7f19-8a5c-4e10-c4ef-1d1b74e88c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a= tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]]) \n",
            "\n",
            "b= tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]]) \n",
            "\n",
            "c= tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de juguete que ilustra como la multiplicación matricial puede ser usada para una adición con pesos\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "\n",
        "print('a=',a,'\\n')\n",
        "print('b=',b,'\\n')\n",
        "print('c=',c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ff4d0f0d742e4f9ba3bcb7f6e8e560ef",
        "deepnote_cell_type": "markdown",
        "id": "FS8asALoLs7K"
      },
      "source": [
        "Notemos que $c$ tiene en cada fila los resultados de los valores acumulados de $b$ según los ponderadores de $a$.\n",
        "\n",
        "El tensor $a$ se interpreta como una matriz de token-a-token y representa la interación/influencia del token en la posición $i$ con el token de la posición $j$. Dado que nuestro modelo es autoregresivo, los tokens del presente solo pueden ser influenciados por tokens pasados, o ellos mismos. Por eso las posiciones de $a$ que cumplen esta restricción $i \\leq j$, son elementos que conforman la matriz triangular inferior de $a$. El resto de las posiciones no tiene influencia sobre los tokens pasados (i.e. 0).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f3c3eaff6931470ab66a89baf7c2b884",
        "deepnote_cell_type": "markdown",
        "id": "XHHT5gRcLs7K"
      },
      "source": [
        "Vamos a crear un _batch_ con datos síntetico de tamaño `B`, donde cada bloque contextual será de largo $T$, y cada _token_ que compone el contexto se representa por $C$ dimensiones (i.e. tamaño del _embedding_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "e2e10fa5f831461daec2ca0c93b27a04",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 25,
        "execution_start": 1693861772506,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6PoLgHVLs7K",
        "outputId": "59eedcc6-c7fc-4be3-d576-3123fccb0bb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ROj2p9WXLs7L"
      },
      "outputs": [],
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cell_id": "2aa3b323c83c4ac4a4f7ea1930ca253c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 16,
        "execution_start": 1693862258062,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CGiGIHpLs7L",
        "outputId": "f197d2c5-d7f8-4aa9-eac8-b9f59c287274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Versión usando softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cell_id": "f89fedd9c5664d448a0f73d90c8c53ca",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 14,
        "execution_start": 1693862260341,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWimixYLs7L",
        "outputId": "d546499b-f674-4d52-d145-1269f5207cb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "wei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f647ef2e2b1d4f589593fb378fe2616f",
        "deepnote_cell_type": "markdown",
        "id": "BXljbLRxLs7L"
      },
      "source": [
        "Los ponderadores anteriores son uniformes, ahora introducimos los conceptos de _queries_ y _keys_ para ver como computar los ponderadores a partir de los datos.\n",
        "\n",
        "* Un _query_ $q(\\cdot)$ corresponde a una proyección lineal de la representación de _embeddings_ de un token particular. Por ejemplo, se proyecto $\\mathbb{R}^{C}\\rightarrow \\mathbb{R}^{H}$.\n",
        "* Los _keys_ es la matriz $K\\in\\mathbb{R}^{T\\times H}$ que contiene proyecciones lineales de todos los _embeddings_ de tokens dentro del contexto, incluído el token que es el query. La proyección lineal de los _keys_ es de igual tamaño (i.e. $H$) que la proyección del _query_.\n",
        "* Los ponderadores para cada _query_ se obtiene a partir de qué tan próximo se relaciona un token respecto al resto de los token dentro de un contexto. Por ejemplo, $q(x_i) \\times K$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cell_id": "a4f9611bdc5f426b87b42952cdf55d51",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-nYTIVhkLs7L",
        "outputId": "59798e85-1ac8-43de-b2e6-8023a071a461"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "Image(url=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2bdc17d026094321b7b1b6b53fd3bec9",
        "deepnote_cell_type": "markdown",
        "id": "8L930wHQLs7M"
      },
      "source": [
        "> b) (Bonus): Explique porqué no se utilizan directamente los _embeddings_ para computar la matriz de atención previo a la normalización, i.e. `X @ X.transpose(-2,-1)`, en vez de usar las proyecciones $QK^\\top$. $X$ es un tensor con dimensiones batch size (B), ventana de contexto (T), dimensiones de embedding (C).\n",
        "\n",
        "Esto es debido a que las proyecciones permiten que el modelo aprenda representaciones más significativas de los tokens de entrada y permite ajustarse a cualquier tarea, ya que se adaptan a través del entrenamiento. Esto es muy importante en tareas de procesamiento de lenguaje natural, ya que el aprender representaciones de buena manera puede tener un impacto significativo en el rendimiento del modelo, además el permitir que el modelo pueda ajustarse a cualquier tarea permite que con, por ejemplo, tareas de traducción de texto, el modelo pueda aprender a enfocarse más en ciertas partes de la secuencia de entrada utilizando la información contextual.\n",
        "\n",
        "Por otro lado, si se utiliza directamente los _embeddings_ se perderán las ventajas ofrecidas por el uso de las proyecciones, en particular no permitiría que el modelo aprendiera representaciones más específicas de los datos de entrada para la atención."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8bdb8dd7d9954b87b248303181315e57",
        "deepnote_cell_type": "markdown",
        "id": "oD_hQMMRLs7M"
      },
      "source": [
        "> c) (Bonus): Explique los argumentos detras de escalar por $1/\\sqrt{d_k}$ referidos en el paper _[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_ (Vaswani 2017).\n",
        "\n",
        "La aplicación del escalado por $1/\\sqrt{d_k}$ asegura que los vectores de peso mantengan una longitud euclidiana de magnitud similar. Esto ayuda a evitar que los pesos de atención se vuelvan excesivamente pequeños o grandes, lo cual puede provocar problemas de inestabilidad numérica o afectar a la capacidad del modelo para converger durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bxejBJsKLs7M"
      },
      "outputs": [],
      "source": [
        "# colocar seed como su RUT\n",
        "torch.manual_seed(206684402)\n",
        "\n",
        "B,T,C = 4,8,32  # batch, time, channels\n",
        "x = torch.randn(B,T,C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ff534d33b5ae46b5a252eaa2023dfa39",
        "deepnote_cell_type": "markdown",
        "id": "aUFyMoSkLs7N"
      },
      "source": [
        "Ejemplo de aplicación de módulo de atención"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cell_id": "64725c524c18451a843b2f837bd65dcd",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUEpv0A3Ls7N",
        "outputId": "e6ffdcb6-c6f6-40c8-a11c-ef9ed8dd4639"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5db2feb142554dddb5ce05738c1f743e",
        "deepnote_cell_type": "markdown",
        "id": "In4PE5RBLs7N"
      },
      "source": [
        "Observaciones:\n",
        "- Atención es un **mecanismo de comunicación**. Puede ser entendido como nodos en un grafo dirigido conectándose unos con otros y agregando información con una suma ponderada de todos los nodos que apuntan a ellos, con pesos dependientes de los datos.\n",
        "- No hay una noción de espacio. Atención simplemente actua sobre el conjunto de vectores. Es por este motivo que se necesitan encoders posicionales.\n",
        "- Cada punto dentro de un batch es, desde luego, procesado de manera independiente y nunca intractua con los otros.\n",
        "- En un bloque de atención \"encoder\" basta comentar la linea que hace masking con `tril`, que hace que los tokens se comuniquen todos con todos. El bloque anterior se llama \"decoder\" porque aplica un masking triangular y se encuentre frecuentemente en configuraciones autoregresivas.\n",
        "- \"auto-atención\" (_self-attention_) sólo signfica que tanto _keys_ como _values_ son producidas desde la misma fuente que las _queries_. En \"atención-cruzada\" (_cross-attention_), las _queries_ vienen de $x$, pero _keys_ y _values_ vienen de otra fuente externa (como puede ser un modulo encoder).\n",
        "- Atención \"escalada\" divide `wei` por $\\frac{1}{\\sqrt{head\\_size}}$. Esto hace que cuando los input $Q$ y $K$ tengan varianza unitaria, `wei` también tendrá varianza unitaria y evitará la saturación de la Softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "833c93c83a6f42e6a9c6e3adce82a4be",
        "deepnote_cell_type": "markdown",
        "id": "l_R99_8ELs7N"
      },
      "source": [
        "> d) (1.5 ptos.) Cree una clase `Head` que implemente un módulo de auto-atención."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cell_id": "e86d22f778c4464894ee70eeb6f47ae1",
        "deepnote_cell_type": "code",
        "id": "iZH4_KcsLs7N"
      },
      "outputs": [],
      "source": [
        "n_embd = 64  # dimensionalidad del input\n",
        "dropout = 0.0\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Una cabeza de auto-atención \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # ------------------------\n",
        "\n",
        "        self.head_size = head_size # Variable que define el tamaño de la cabeza de atención\n",
        "\n",
        "        # Inicialización las matrices de key, query y value respectivamente, mediante el uso de capas lineales\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # ------------------------\n",
        "        # HINT: cuando aplique tril, ocupe self.tril se define automaticamente\n",
        "        # al instanciar\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input size (batch, time-step, channels)\n",
        "        # output size (batch, time-step, channels)\n",
        "        B,T,C = x.shape\n",
        "        # -----------------------------------------\n",
        "\n",
        "        # Aplicación de las matrices de pesos sobre la secuencia de entrada,\n",
        "        # se generan las secuencias de key, query y values respectivamente\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "\n",
        "        # Computar los score de atención (\"affinities\")\n",
        "\n",
        "        # Se calculan los productos punto entre querys y keys\n",
        "        wei = (q @ k.transpose(-2, -1))/(self.head_size**0.5)  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        # Se aplica la máscara triangular inferior\n",
        "        wei = wei.masked_fill_(self.tril == 0, float('-inf'))   # (B, T, T)\n",
        "\n",
        "        # Se aplica la función Softmax para obtener las probabilidades o pesos de atención\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "        # Se aplica el método de Dropout\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Adición (con pesos) de las atenciones\n",
        "        # Se realiza el producto entre los pesos de atención y la secuencia de value\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        # --------------------------------------\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "96c2b9cefd6b453cb437392b7f03f5d4",
        "deepnote_cell_type": "markdown",
        "id": "8XRKFaTcLs7N"
      },
      "source": [
        "La arquitectura decoder del paper Transformer implementa varias versiones de _self-attention_ en paralelo, cada una es una \"cábeza de atención\", y estas concatenan sus resultados en un modulo conocido como `MultiHeadAttention`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cell_id": "214c8ece545f4817b8561dfae0db408a",
        "deepnote_cell_type": "code",
        "id": "pe34LAaDLs7O"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Múltiples cabezas de auto-atención en paralelo \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5d5e70b87ba948e6b2db556e91caeac7",
        "deepnote_cell_type": "markdown",
        "id": "owcvPB36Ls7O"
      },
      "source": [
        "> e) (0.75 pto.) Implemente una clase `FeedForward` como se describe en el artículo [\"Attention is all you need, Vaswani et al.\"](https://arxiv.org/pdf/1706.03762.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cell_id": "2bd2438a5180464c873da561e6a8527c",
        "deepnote_cell_type": "code",
        "id": "hYgRufrFLs7O"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"\n",
        "        Implementar FeedForward descrita en sección:\n",
        "         \"3.3 Position-wise Feed-Fordward Networks\", paper\n",
        "         \"Attention is All You Need\"\n",
        "        https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "        in: n_embd\n",
        "        out: n_embd\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # -------------------\n",
        "\n",
        "            # La red de Feed-Forward consiste en dos transformaciones lineales\n",
        "            # con una función de activación ReLU en medio de las dos.\n",
        "\n",
        "            # La primera transforamción lineal aumenta el tamaño del input en 4\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            # La segunda transformación lineal vuelve a la dimensionalidad original\n",
        "            nn.Linear(4 * n_embd, n_embd)\n",
        "\n",
        "            # ------------------\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cell_id": "fd893f1e2d6842b08beb90dd1091e678",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "534uw6AhLs7O",
        "outputId": "e3690915-e905-400f-98b2-e638978e8fc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "Image(url=\"http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e0384b1c93274e7abfb20d5627b2958c",
        "deepnote_cell_type": "markdown",
        "id": "-X403BePLs7O"
      },
      "source": [
        "> f) (0.5 ptos.) Explique la relación entre los hiperparámetros `n_head` y `head_size` de la clase `MultiHeadAttention`. Piense en su rol dentro del bloque decoder (i.e. atención + feedforward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ28iKtpLs7P"
      },
      "source": [
        "Respuesta: Al analizar la clase `MultiHeadAttention` se puede observar que la entrada se divide en `n_head`cabezas de atención independientes, donde cada una de ellas tiene su propio conjunto de proyecciones(Q, K y V) que se aplican a la entrada original. La dimensionalidad de las proyecciones Q, K y V en cada cabeza de atención es controlada por el hiperparámetro `head_size`. Por lo tanto, cada cabeza de atención tiene una dimensión `head_size` para sus proyecciones.\n",
        "\n",
        "Además, dentro de cada cabeza de atención, se realiza la atención entre las proyecciones Q, K y V y se obtienen las salidas ponderadas. Luego, las salidas de todas las cabezas de atención se concatenan para formar una representación combinada que tiene una dimensión total de n_head * head_size.\n",
        "\n",
        "Por otra parte, en la capa de feedforward, la entrada es el resultado que proviene de la atención multi-cabeza, al cual la capa de feedforward aplica transformaciones lineales y transformaciones no lineales como funciones de activación(ReLU) para obtener las representaciones finales antes de la salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1GdAHRsLs7P"
      },
      "source": [
        "Ahora procederemos a armar la clase que implemente un bloque de Decoder. Nótese que en realidad estamos codificando texto con este bloque, pero esto corresponde a la parte \"Decoder\" del Transformer original, por ende guardamos esa nomenclatura. La motivación del uso del decoder es modelar el texto de maner auto-regresiva, que es ideal para la generación de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "_CeSRpsVLs7P",
        "outputId": "3b0cd934-b3d9-4aac-ca31-61a6dd46870a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://i.stack.imgur.com/bWnx0.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "Image(url='https://i.stack.imgur.com/bWnx0.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "708d555c66ac4f4a940e82ab2251cb76",
        "deepnote_cell_type": "markdown",
        "id": "hZNQddpsLs7P"
      },
      "source": [
        "> g) (0.75 ptos.) Complete el paso _forward_ de la clase `DecoderBlock`. Recuerde en particula incorporar las conexiones residuales (_skip connections_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cell_id": "8a013f2aceba4d7eb70390a12f21c308",
        "deepnote_cell_type": "code",
        "id": "eNP2APiRLs7P"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"  BloqueTransformer: COMUNICACIÓN seguida de CÓMPUTO \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: dimensión de embeddings, n_head: número de cabezas de atención\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hint: aplique las capas de normalización siempre antes de otra capa (Pre-LN varaint)\n",
        "        # Ver: https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure\n",
        "        # ---------------------\n",
        "\n",
        "        # Aplicación de primera capa de normalización antes de la atención\n",
        "        ln1_output = self.ln1(x)\n",
        "\n",
        "        # Aplicación de capa de atención multi-cabeza\n",
        "        sa_output = self.sa(ln1_output)\n",
        "\n",
        "        # Conexión residual con la entrada original\n",
        "        x = x + sa_output\n",
        "\n",
        "        # Aplicación de segunda capa de normalización antes de la atención\n",
        "        ln2_output = self.ln2(x)\n",
        "\n",
        "        # Aplicación de capa de FeedForward\n",
        "        ffwd_output = self.ffwd(ln2_output)\n",
        "\n",
        "        # Conexión residual con la salida de la capa de FeedForward\n",
        "        x = x + ffwd_output\n",
        "\n",
        "        # ----------------------\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cd75ed98e96247ffa253556488b53e4e",
        "deepnote_cell_type": "markdown",
        "id": "Z0IJc5WYLs7P"
      },
      "source": [
        "## 5. Modelo GPT: Juntando todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00e10b6c07f5442b95e5c2b0819edc0a",
        "deepnote_cell_type": "markdown",
        "id": "cmKSx9-7Ls7P"
      },
      "source": [
        "> h) (1 pto.) Complete el código de la clase `GPTLanguageModel`procesando adecuadamente el input del modelo. Complete además el método `generate` para samplear elementos que completen auto-regresivamente una secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cell_id": "88ed14ab007b4feea097aaa558f1cb73",
        "deepnote_cell_type": "code",
        "id": "ZZDarHcjLs7P"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # cada token lee directamente los logits para el token siguiente de una lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx y targets son ambos tensores (B,T) de enteros\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "\n",
        "        # -----------------------------\n",
        "\n",
        "        # Combinación de los embeddings de token y posición\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        # Procesamiento de x a través de los bloques\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Aplicamos la última capa de normalización\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Calculo de logits\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        # -------------------------------\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx es un arreglo (B, T) de índices en el contexto actual\n",
        "        for _ in range(max_new_tokens):\n",
        "            # restringir idx a los últimos block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # obtener las predicciones\n",
        "            logits, loss = self(idx_cond)\n",
        "            # enfocarse sólo en el último paso\n",
        "            logits = logits[:, -1, :]  # se convierte en size (B, C)\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "            # Aplicación de función Softmax para obtener las probabilidades\n",
        "            probs = F.softmax(logits, dim=-1)  # tensor de dimensionalidad (B, C)\n",
        "\n",
        "            # Sampleo de la dsitribución multinomial usando las probabilidades\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # tensor de índices de dim (B, 1)\n",
        "\n",
        "            # Actualización del índice a la secuencia actual\n",
        "            idx = torch.cat([idx, idx_next], dim=1)  # tensor resultante de dimensionalidad (B, T+1)\n",
        "\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cell_id": "56fa2d0e8f424060841dfb5a0793b8e8",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo_xXniJLs7Q",
        "outputId": "255955e4-915b-4a51-c05f-9c1beadf3373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de parámetros del modelo: 10.844297 millones\n"
          ]
        }
      ],
      "source": [
        "# Definimos hiperparámetros (global variables, esto se puede hacer mucho mejor)\n",
        "\n",
        "batch_size = 64 # cuantos secuencias de fragmentos del corpus procesaremos de manera independiente (aka B)?\n",
        "block_size = 256 # cuál será el tamaño del bloque de contexto a considerar para predecir (aka T)?\n",
        "max_iters = 8000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4  # modelo de tamaño pequeño probar esta lr por defecto\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384  # se debe considerar en conjunto con n_head, según análisis en (f)\n",
        "n_head = 6    # se debe considerar en conjunto con n_embd, según análisis en (f)\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "model.to(device)\n",
        "\n",
        "# printear el número de parámetros del modelo\n",
        "print('Número de parámetros del modelo:', sum(p.numel() for p in model.parameters())/1e6, 'millones')\n",
        "\n",
        "# definir el optimizador de PyTorch\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTwWKKMrLs7Q"
      },
      "source": [
        "Definiremos una función para generar batches de secuencias a partir de nuestro corpus de entrenamiento o validación. Además de una función para obtener estimaciones de la función de costo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UzrzXC7_Ls7Q"
      },
      "outputs": [],
      "source": [
        "# definimos un \"DataLoader\"\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# definimos una función para obtener estimados de nuestras\n",
        "# pérdidas tanto en los conjuntos de train como en val\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olGc9diCLs7Q"
      },
      "source": [
        "Primer sanity check, obtener batches con dimensiones correctas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Figu5MP1Ls7Q",
        "outputId": "379e9fe9-352b-4615-8e82-c5dea37aa1e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 256]), torch.Size([64, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Verifiquemos que las dimensiones sean correctas\n",
        "xb, yb = get_batch('train')\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rTjp07pLs7Q"
      },
      "source": [
        "Segundo sanity check, verificar que la data fluya correctamente por el modelo,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76IGSfLELs7R",
        "outputId": "0dfb1ce4-6b7f-4e06-9616-4f3181fbaad9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 256, 137])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Verifiquemos que el forwardpass del modelo no tenga problemas\n",
        "model(xb)[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6cd551bb57f54567a9780b868c2081e7",
        "deepnote_cell_type": "markdown",
        "id": "-tBGUnZ0Ls7R"
      },
      "source": [
        "> i) (1 pto.) Complete el bucle de entrenamiento usando comandos de `pytorch.optimize` conocidos. Ejecute el entrenamiento (se recomienda dejarlo corriendo e ir hacer algo más...). Corrobore que se modelo es capaz de generar texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cell_id": "faa0a3b9283247f1a8cf6d6b8d599959",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6GmMctpLs7R",
        "outputId": "fe628eb0-0c5f-4f8e-bcad-97c1bbe14f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.9346, val loss 4.9391\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t论MtUqsq,子制7最^:9的\\}米F'hE全米Q件最论]”1^社Gx><论\f为,!5T`v1Q米hj-自UC\\'做Y>|坛格\\\t自’k论R】W;&3Nd.】自d’‘3式本j<【做Y志UCCo-子l\n",
            "step 500: train loss 1.5831, val loss 1.6544\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUVeard.ILriykewitgle twenstaille tekor in upneesched as agre Of Pernofess\n",
            "Ruckus at Harry,tilone\"ll \n",
            "step 1000: train loss 1.2686, val loss 1.3673\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe and havethe only, mit seemsely id hoped in Ron hige. Her. . Ron isn an erthudgener toward your w\n",
            "step 1500: train loss 1.1704, val loss 1.2660\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t- Gin Fleeryellet on spligh-blursed.i. He son fare and Hogward at fiftet and there and -- he stopped\n",
            "step 2000: train loss 1.1205, val loss 1.2252\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsa shummbard,awnd in a magazing which so take out.\n",
            "　　That really for within surra!\" he could. Ron ha\n",
            "step 2500: train loss 1.0790, val loss 1.2004\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\n",
            " He to,ice bento Harry was look! Paning for and loudly to a bottle. Ron looked back at Harry's hea\n",
            "step 3000: train loss 1.0556, val loss 1.1784\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tHar-crurinin, souriary curleson bat for his armchair. Dumbledoric's tall aliery open. \n",
            "\n",
            "COde on his \n",
            "step 3500: train loss 1.0321, val loss 1.1636\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Heold in anoff whimpered or delate Bore and bows. \n",
            "Ginnejump, merely merely coming from the paired \n",
            "step 4000: train loss 1.0157, val loss 1.1560\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Harryied back_\n",
            "Bagmadly,Bloo.\n",
            "\"He's alle! Ginny. He litter! let it back and lained it along nexious\n",
            "step 4500: train loss 0.9984, val loss 1.1434\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Harr--=\n",
            "　　He spun his hand all his stirst on the floor.\n",
            "　　It was a larap of Appalaminating in Sea V\n",
            "step 5000: train loss 0.9861, val loss 1.1358\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\n",
            " Si242o am along lascushing McLam rose, but the collar surface of the potion in a yell (and it tur\n",
            "step 5500: train loss 0.9714, val loss 1.1303\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRon \n",
            "|38687%5\n",
            "　*116|))\n",
            "　　2027\n",
            "　　Auaaking Noiches streaming made a couple of its over consinx. Behind\n",
            "step 6000: train loss 0.9565, val loss 1.1273\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tVe com-allouradfule suffiliaracc;s fatalles flying its Muggles volumi-ship. Master: Ernia,e and dare\n",
            "step 6500: train loss 0.9453, val loss 1.1166\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            " Dineezt> \n",
            " Ron-supepaleadears \n",
            "Herepouse A part of poisting his pumpinous \n",
            "potesters s-“ \n",
            "\n",
            " Then V\n",
            "step 7000: train loss 0.9316, val loss 1.1159\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            " — Shaill!=-jine —? a \"House-\"\n",
            "Blecome striwhill lay. But Dudley shudders into her whistled hoat an\n",
            "step 7500: train loss 0.9222, val loss 1.1095\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "76 VI dodst hoote with flossescares! pummaly! \n",
            "\"I — it — ac-red! \n",
            "horses to white a house, touched;\n",
            "step 7999: train loss 0.9119, val loss 1.1083\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\n",
            " And Deatthinelwhithe direcelleverions high werea gonarounchair leap in Death Eaters nervously. As\n"
          ]
        }
      ],
      "source": [
        "# colocar RUT como semilla\n",
        "torch.manual_seed(206684402)\n",
        "\n",
        "# comenzamos el training loop...\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # de vez en cuando evaluar la loss en los conjuntos de train y evaluación\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        context = torch.zeros((1, 256), dtype=torch.long, device=device)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(\"Testing text generation:\")\n",
        "        print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "    # samplear un batch de datos\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # ----------------------------------------\n",
        "\n",
        "    # Activación del optimizador\n",
        "    # Limpieza de los gradientes acumulados\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calcula los gradientes aplicando el paso backward con el resultado de la función loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Actualización de los parámetros del modelo\n",
        "    optimizer.step()\n",
        "\n",
        "    # ----------------------------------------\n",
        "\n",
        "# Se especifica la ruta y el nombre del archivo donde se quiere guardar el modelo\n",
        "\n",
        "path = '/content/modelo_gpt.pth'\n",
        "\n",
        "# Se guarda el modelo completo y entrenado, tanto la arquitectura como los parámetros\n",
        "\n",
        "torch.save(model, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5a8d031142ab45d48e9b9eeb81f6b2f7",
        "deepnote_cell_type": "markdown",
        "id": "odoiAapmLs7R"
      },
      "source": [
        "## 6. Generando secuencias de texto con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se especifica la ruta y el nombre del archivo con el modelo que se quiere cargar\n",
        "\n",
        "path = '/content/modelo_gpt.pth' # Se debe tener el archivo en el directorio indicado, si es distinta la dirección se debe editar\n",
        "\n",
        "# Se carga el modelo completo\n",
        "\n",
        "model = torch.load(path)\n",
        "\n",
        "# Se establece el modo evaluación\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk-hwSmXQ478",
        "outputId": "b8e212f8-9f18-4167-ae32-1eae031421d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(137, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=137, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "cell_id": "9e6d66e2a3ba49a4ae3c23d4e65d57df",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WShw9ZUoLs7R",
        "outputId": "dc61169d-463c-436a-c715-f9d409b2ddae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "802 He saideteseadis \n",
            "\n",
            "  \n",
            "\n",
            " “Hothappenet to you. . Wouldn’ve been on \n",
            "your oldest?” \n",
            "\n",
            " “I don’t care that boy smart!” \n",
            "\n",
            " Both Ron and Hermione had to look so dis-solved. \n",
            "\n",
            " “My Lord,” Harry heard something on the table; he was sure it was hammering \n",
            "\n",
            "Xenophilius Flamel, blinking his, into a bowl of your rest instead, would never a \n",
            "sound of telephone in Mr. \n",
            "\n",
            "\f\n",
            "Cleanen this meant arrange that everyone are. There was long them beside Scrimgeour. He learned them all around on his \n",
            "matter? Where h\n"
          ]
        }
      ],
      "source": [
        "# Generar usando el modelo\n",
        "context = torch.zeros((1, 256), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "# Para escribir en un archivo\n",
        "# open('output.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prueba con prompt escrito\n",
        "prompt = torch.tensor(encode(\"Felipe Tobar, the new Defence Against the Dark Arts teacher, presented his new spell to his students. It was called MCMC Algorithm, which was known to be an attack so powerful that no one could survive it. Even the wizard Andrew Ng couldn't fight its power\"), dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(decode(model.generate(prompt, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VvteADJyudP",
        "outputId": "698a632d-4b70-4cda-a284-f8d8e756e9bd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Felipe Tobar, the new Defence Against the Dark Arts teacher, presented his new spell to his students. It was called MCMC Algorithm, which was known to be an attack so powerful that no one could survive it. Even the wizard Andrew Ng couldn't fight its powerful.\n",
            "　　\"WE won't go a letter, it's good,\" he was hoisted him. \"What was that?\"\n",
            "　　\"What?\" said Harry in a very interest. \"Good thinking dark. . .\n",
            "　　But then - I'll add to them -\"\n",
            "　　\"I don't like that news,\" said Dumbledore. \"Flee was better talk to you to King's Cross St at all the other Minister's job, that Harry is not our telling Dark as much more as I know that. You are sure ere there.\"\n",
            "　　\"About your way,\" said Dumbledore. \"You see? Harry Potter blinks it weren't my office. Think on our Knowing Crouch is affects that. You always have Filch sayed it in Dumbledore's a fat of old, but not even if you have already the one you will tell me you that might be at the hint of a panicg in the back of you. Dumbledore will tell you have decided to dwin, discovered, sir? The man seats in a very fast, that cheers were the Oliver Wizarding Patronus Care of Secrets?\"\n",
            "　　\"Be with them,\" said Harry.\n",
            "　　Stan had not speak; they could hear them before him, and at last, Harry could not say \"Bathilda?\"\n",
            "　　\"If someone's got the one who shriveled away. You haven't forgotten.\" He trailed up.\n",
            "　　\"Er - be,\" said Ron in a tragedy voice, racing with a small wand.\n",
            "　　\"Right,\" she said brightly. \"Look at the man. Did Hagrid, who seem respectful, it's all ritable the next week's dept communicate, see, the Remembra Remus Luna Loved.\n",
            "　　I have told Aragog's hand out here works for the Ministry,\" she asded urgently, looking strangely ratted.\n",
            "　　\"You don't see another party?\" Hermione asked, and her voice came nasty.\n",
            "　　\"When the Ministry meets them?\" said Bagman quietly.\n",
            "　　\"All right then, we won't have twelve them to go watch, I'm holding ut them washing rather the slightest and --\"\n",
            "　　\"We really leave this view,\" said Mr. Weasley, cruel-like Albus Dumbledore. \" Harry's brothers placed up to the stories. He had been burned into his bag.\n",
            "　　\"Me -- I told you this year.\"\n",
            "　　\"So, \" said Ron.\n",
            "　　When disappeared to him as he stood in the starlight sunlight, she kept out of the dungeon.\n",
            "　　\"I'll be at the tale of the weekend,\" Mrs. Weasley gasped. \"They're still keeping out in briefder when the disturbed at once.\"\n",
            "　　\"My owl's sister,\" said Harry docuringly. He was really suspicious, he did not want to cut Ron in for bed, but he kept ushing himself to the better - \"\n",
            "　　\"No problem,\" Hermione told Harry, awre that Hagrid was waving her face bent like an old Death Eater's.\n",
            "　　\"Yeah, others he was a bite of the team that became left them pulled freedom through the now and asked that making they rome to walk across the grounds. A silent huge of mangled in the Great Hall Gryffindor version. It seems worried that powerful toner of our Morphmai, a maid recent of the good tests: the Beauxbatons without what I was from the Minister of Magic homework; you won finding them about it like the Squibbler. Moody would bear somebody like a pans about record?\" Bagman asked tentatively.\n",
            "　　\"Perhaps Moody's dear assurance,\" said a bringing noiseless prod over his socks. \"He's supposed to fun gold. Had he is not yet you'd thought one who are, Not playing him off his vision, or are that no alternative words.\"\n",
            "　　Harry had never given a spateamed escape, Cornelius, and Flamel Weasley were soon zere passing from him, Dean Thomas's and Zabins and Stunning his broomstick fell, he kindly in front of himself and Harry's T-shirt did not say as the twins and a hidden from with the lead ' head, propped into an alternate hubbugs. He received it, fumbled in searchd out of the cabin, remained silvery face, and with slimy green eyes, his veins, whose Terrania had cried his grin. When none of them started to talk to her, she was scared of the newsord of socks, and their knot was -- if she even liked this thing that, at that moment, Harry pulled out his hand and Ron into the bit of his life open. How he had the founders from his pockets? \"Gives your a joke, Which is Saturday, Greyback? Broken had remembered this place?\" \"That's Very Wimbledore!\" Harry charted. Aragog's Mef-\n",
            "　　Harry told her he was reading his fest of all longer than anything to should be there. ... Seamus was like him, pair of conducts famously in secrets, over, which he never had it to work: well, Harry. ....\n",
            "　　Everyone waited to you, Sirius to common make him best: Then the human on the edurse had been in an empty candlelabrack. Harry could hear rushing, but he had been very dreamy when Mr. Weasley thought.\n",
            "　　\"Harry, I'm sorry, if you were having this,\" said Hermione, pointing, \"we were still waiting for the time. Now Neville turned it. My father. You loads of the Decoy Defeat Arts ... damn was scheduless. .. you know -quite magick. ...\"\n",
            "　　Hagrid looked straight at the pantry, exchanging the moment as though the faces were spinning, with more needing things at all. They always in the subject. Their infact was offensive . . . they drifted back to the ladder, following Percy ... and he heard them back...\n",
            "　　Cedric took a deep breath.\n",
            "　　Harry touched him, it was slightly and pilled it like a mousy ghost only. When the memory had op\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos un archivo de texto a partir del prompt\n",
        "open('output.txt', 'w').write(decode(model.generate(prompt, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaSFgKBg1Ozj",
        "outputId": "f0de67b7-16c2-41c1-d20d-b59011a467a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10256"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpLeej5Ls7R"
      },
      "source": [
        "> j) (bonus) Define un training loop para el baseline (modelo bi-grama). Entrénelo usando un npumero similar de épocas y compare las losses y generación de texto con su modelo anterior."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "409cf1afb7654c8282dccc46f347e63f",
    "deepnote_persisted_session": {
      "createdAt": "2023-09-04T22:31:43.624Z"
    },
    "kernelspec": {
      "display_name": "mlstack",
      "language": "python",
      "name": "mlstack"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}