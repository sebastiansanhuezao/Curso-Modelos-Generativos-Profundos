{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3ec1cec624ef406c8f6a8d3cb7b93d46",
        "deepnote_cell_type": "markdown",
        "id": "e4Pr6pv5ie50"
      },
      "source": [
        "**MDS7203 Modelos Generativos Profundos, Primavera 2023**\n",
        "\n",
        "# Laboratorio 2: Modelo de lenguaje auto-regresivo\n",
        "\n",
        "**Profesor**: Felipe Tobar, **Auxiliares**: Crist√≥bal Alc√°zar, Camilo Carvajal Reyes, **Ayudante**: Joaqu√≠n Barcel√≥.\n",
        "\n",
        "**Fecha de entrega**: viernes 29 de septiembre 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "38cd31c023b047f080018f94f24927e8",
        "deepnote_cell_type": "markdown",
        "id": "N08HpQApLs6x"
      },
      "source": [
        "**Nombre: Sebasti√°n Sanhueza**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "279c16d9f94a4b5dbe45f9f4f6ff6f07",
        "deepnote_cell_type": "markdown",
        "id": "Xjt-hcwhLs6x"
      },
      "source": [
        "**Instrucciones**: El presente notebook contiene enunciado e instrucciones para la realizaci√≥n del laboratorio. Usted deber√° completar los c√≥digos (en este archivo o en una copia del mismo) donde se le pida hacerlo. Usted deber√° entregar el notebook con sus respuestas, cumpliendo lo siguiente:\n",
        "\n",
        "- Los comentarios en c√≥digo deben ser concisos pero claros. No se evaluar√°n sub-preguntas donde solo exista c√≥digo sin comentarios pertinentes.\n",
        "- El c√≥digo debe ser ordenado y ejectuable. No se evaluar√°n notebooks o scripts que generen errores en su ejecuci√≥n. Se aconseja resetear la kernel y corroborar la correcta execuci√≥n de todas las celdas antes de ejecutar el entrenamiento de su modelo.\n",
        "- Si bien se aconseja el uso de internet y otras herramientas para asistir su trabajo, asi como discusiones con el ED y estudiantes, el c√≥digo que entregue debe ser de su autor√≠a.\n",
        "\n",
        "El objetivo del laboratorio ser√° implementar, desde casi cero, un modelo de lenguaje estilo GPT, i.e., basado en el uso de un bloque \"decoder\" de la arquitectura Transformer (como se muestra en la imagen a continuaci√≥n). Este tipo de modelos es un ejemplo de modelo auto-regresivo y que ha tenido gran relevancia en el √∫ltimo tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "-NgdWMHtLs6y",
        "outputId": "a4f0843d-46ef-4864-e981-9d7a736f6f07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://i.stack.imgur.com/bWnx0.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://i.stack.imgur.com/bWnx0.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzr2TNahLs6z"
      },
      "source": [
        "Algunos links √∫tiles:\n",
        "\n",
        "* [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "* [GPT2, original blog post](https://openai.com/research/better-language-models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "40d3aa47f3204ca888f874beb273d9f5",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "vsiWqJQsLs6z"
      },
      "source": [
        "### Resumen de preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "14369c673f3049a19e647a78a00686d4",
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "m2Jcqz9nLs60"
      },
      "source": [
        "- [ ] a) (0,5 ptos.) Definici√≥n de diccionarios para vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c3ae5920aed6440ea4c012068c08bb8b",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "9sqO5_aZLs60"
      },
      "source": [
        "- [ ] b) (bonus) Utilizaci√≥n de embeddings previo a la normalizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d259ba9b76b74b2e8eb0132b06f7f31c",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "lMC59nHaLs60"
      },
      "source": [
        "- [ ] c) (bonus) Escalamiento por $1/\\sqrt{d_k}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "14547d4ff1564ceca8f1133b2bf3831f",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "Uory2a5QLs60"
      },
      "source": [
        "- [ ] d) (1.5 ptos.) Creaci√≥n de clase `Head`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "806e04b64a5f4e8c9c46b8addb61da30",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "YAVxRGCcLs61"
      },
      "source": [
        "- [ ] e) (0.75 pto.) Implementaci√≥n de clase `FeedForward`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "af2c2194e9e94407b2ed8d38ac7dd9c5",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "r8Vn53TELs61"
      },
      "source": [
        "- [ ] f) (0.5 ptos.) Relaci√≥n entre hiper-par√°metros n_head y head_size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9cec4a573e824ff2a535781db9773328",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "0XGUGfNwLs61"
      },
      "source": [
        "- [ ] g) (0.75 ptos.) Forward pass en `DecoderBlock`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "33851255223c4c569a4ef98add7b0b4f",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "WokI6TIILs61"
      },
      "source": [
        "- [ ] h) (1 pto.) Implementaci√≥n clase `GPTLM`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "273f32755bf446cba8132463ead5c3ac",
        "checked": false,
        "deepnote_cell_type": "text-cell-todo",
        "formattedRanges": [],
        "id": "zdEyPdMpLs61"
      },
      "source": [
        "- [ ] i) (1 pto.) Training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkVMwOPKLs62"
      },
      "source": [
        "- [ ] j) (bonus) Comparar modelo con Baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "3381cfa7678849408eeaf9aedb7fb2d1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 5134,
        "execution_start": 1693917871424,
        "id": "xIPkmAHJmC9s",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ZwlYE_Ls62"
      },
      "source": [
        "Caracter√≠sticas de la GPU, si es que est√° disponible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6593dVMLs62",
        "outputId": "10f8e327-1cf7-403b-9197-8311aba3dca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 30 05:13:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a607d5c593c64504a2c445b78a01b637",
        "deepnote_cell_type": "markdown",
        "id": "O54vJC-DjbMe"
      },
      "source": [
        "## 1. Corpus üìñ\n",
        "\n",
        "Escoja uno de los dos datasets:\n",
        "- shakespeare.txt: concatenaci√≥n de obras de shakespeare, ~ 1 millon de caracteres\n",
        "- cabromagico.txt: concatenaci√≥n de los libros de Harry Potter, ~ 6 millones de caracteres\n",
        "\n",
        "Escoja en base a sus gustos y capacidades de c√≥mputo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x0TnAbzLs63",
        "outputId": "142c124e-1859-4120-f1ea-364b435df000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-30 05:13:56--  https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/cabromagico.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6425820 (6.1M) [text/plain]\n",
            "Saving to: ‚Äòcabromagico.txt‚Äô\n",
            "\n",
            "cabromagico.txt     100%[===================>]   6.13M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-09-30 05:13:57 (70.6 MB/s) - ‚Äòcabromagico.txt‚Äô saved [6425820/6425820]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Si usan collab, descargar dataset desde repo GAMES descomentando una de las siguientes lineas:\n",
        "# !wget https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/shakespeare.txt\n",
        "!wget https://raw.githubusercontent.com/GAMES-UChile/Curso-Modelos-Generativos-Profundos/main/labs/data/cabromagico.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc5zIGDoLs63",
        "outputId": "e37009f9-cbba-42f3-f955-1837cd91cc3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tama√±o del corpus (cabromagico.txt): 6,340,988 caracteres\n"
          ]
        }
      ],
      "source": [
        "# Selecciona el corpus a su gusto\n",
        "#filename = 'shakespeare.txt'\n",
        "filename = 'cabromagico.txt'\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(f\"Tama√±o del corpus ({filename}): {len(text):,} caracteres\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxRieasJLs63",
        "outputId": "14117757-80b5-4336-d07c-1f18d4fff7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            "\f\u001f !\"$%&'()*,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|}~√©‚Äì‚Äî‚Äò‚Äô‚Äú‚Äù‚Ä¶„ÄÄ„Äê„Äë‰∏ã‰∏∫‰π¶‰ª∂‰Ωú‰Ω†ÂÅöÂÖ®Âà∂Âå∫ÂùõÂ≠êÂºèÂøóÊÇ®ÊñáÊñ∞ÊúÄÊú¨Êù•Ê†ºÁîµÁöÑÁ§æÁ´ãÁ±≥Á≥ØËá™Ë¶ÅËÆ∫ËΩΩ\n",
            "137\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "063a10ca987f4b479fb3b53b92f2ad52",
        "deepnote_cell_type": "markdown",
        "id": "Dv_9XZrpLs64"
      },
      "source": [
        "> a) (0.5 ptos.) Dada una lista de ordenada de caracteres, defina:\n",
        "> - stoi: un diccionario caracter -> √≠ndice\n",
        "> - itos: un diccionario √≠ndice -> caracter\n",
        "> Con lo anterior, defina dos funciones encode y decode que tomen un string y una lista de √≠ndices respectivamente y devuelvan una lista de √≠ndices y un string seg√∫n corresponda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "599ecea77d984abaace18d2f1cc934f1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 9,
        "execution_start": 1693859157194,
        "id": "NURlEkGjkSEf",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------\n",
        "\n",
        "# Diccionario tipo caracter:√≠ndice\n",
        "stoi = {char:index for index,char in enumerate(chars)}\n",
        "\n",
        "# Diccionario tipo √≠ndice:caracter\n",
        "itos = {index:char for index,char in enumerate(chars)}\n",
        "\n",
        "# encoder: toma un string, devuelve una lista de √≠ndices\n",
        "encode = lambda s: [stoi[element] for element in s]\n",
        "\n",
        "# decoder: toma una lista de √≠ndices, devuelve un string\n",
        "decode = lambda l: ''.join([itos[element] for element in l])\n",
        "\n",
        "# --------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fveui6wELs64"
      },
      "outputs": [],
      "source": [
        "if filename == 'shakespeare.txt':\n",
        "    assert encode('hola, que tal?') == [46, 53, 50, 39, 6, 1, 55, 59, 43, 1, 58, 39, 50, 12], 'Verifica que el output entregue una lista de enteros'\n",
        "    assert decode(encode('hola, que tal?')) == 'hola, que tal?', 'Debe ser un string'\n",
        "elif filename == 'cabromagico.txt':\n",
        "    assert encode('hola, que tal?') == [73, 80, 77, 66, 14, 4, 82, 86, 70, 4, 85, 66, 77, 33], 'Verifica que el output entregue una lista de enteros'\n",
        "    assert decode(encode('hola, que tal?')) == 'hola, que tal?', 'Debe ser un string'\n",
        "else:\n",
        "    print('Estas usando un filename distinto para construir el corpus!\\nSi estas explorando otro corpus esta bien!\\nRecuerda verificar que la funcionalidad esta correcta con shakespear o cabromagico.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "488abcf1ec984a438cab8626fc6e8160",
        "deepnote_cell_type": "markdown",
        "id": "ktvBOC1qmJDY"
      },
      "source": [
        "Nuestro modelo no entiende el lenguaje directamente, sino que los representa como n√∫meros. Pasamos el corpus completo a su representaci√≥n de enteros, usando el `stoi` (aka _string-to-index_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cell_id": "9e79dd31677a477580450a4b80e2aa95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 136,
        "execution_start": 1693859161854,
        "id": "WAsyhejrl2X6",
        "outputId": "01a9bc18-63d4-4ae8-8660-332d83a8c5ef",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6340988])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "39c418201e2a4534a13ddc5527929041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "d9aOmlLVlEcr",
        "outputId": "27575047-af89-4f88-8377-3930f9cbf90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto con los primeros 100 caracteres:\n",
            "----------------------------------------------\n",
            "\n",
            "Harry Potter and the Sorcerer's Stone\n",
            "CHAPTER ONE\n",
            "THE BOY WHO LIVED\n",
            "Mr. and Mrs. Dursley, of number \n",
            "\n",
            "----------------------------------------------\n",
            "Su representaci√≥n como tensor de PyTorch...\n",
            "\n",
            "tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70, 83,  4, 66, 79, 69,  4, 85,\n",
            "        73, 70,  4, 52, 80, 83, 68, 70, 83, 70, 83, 10, 84,  4, 52, 85, 80, 79,\n",
            "        70,  1, 36, 41, 34, 49, 53, 38, 51,  4, 48, 47, 38,  1, 53, 41, 38,  4,\n",
            "        35, 48, 58,  4, 56, 41, 48,  4, 45, 42, 55, 38, 37,  1, 46, 83, 16,  4,\n",
            "        66, 79, 69,  4, 46, 83, 84, 16,  4, 37, 86, 83, 84, 77, 70, 90, 14,  4,\n",
            "        80, 71,  4, 79, 86, 78, 67, 70, 83,  4])\n"
          ]
        }
      ],
      "source": [
        "N=100\n",
        "print(f\"Texto con los primeros {N} caracteres:\\n----------------------------------------------\\n\")\n",
        "print(text[:N])\n",
        "print(\"\\n----------------------------------------------\\nSu representaci√≥n como tensor de PyTorch...\\n\")\n",
        "print(data[:N])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5aa3280c539a40c7a350543d116126c6",
        "deepnote_cell_type": "markdown",
        "id": "Aqdw5-8BnVhX"
      },
      "source": [
        "## 2. Separar el dataset üî® y üéì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "4ee3ee4d9f4c425e86da95a874ce263f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 14,
        "execution_start": 1693859215823,
        "id": "zzUjG0OqnWr4",
        "outputId": "e9bff93f-37b1-4382-c3ea-cba972d61805",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Tama√±o del corpus de entrenamiento: 5,706,889 (0.90) caracteres\n",
            "--> Tama√±o del corpus de validaci√≥n: 634,099 (0.10) caracteres\n"
          ]
        }
      ],
      "source": [
        "n = int(0.9 * len(data))  # 90%\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(f\"--> Tama√±o del corpus de entrenamiento: {len(train_data):,} ({(train_data.shape[0] / data.shape[0]):.2f}) caracteres\")\n",
        "print(f\"--> Tama√±o del corpus de validaci√≥n: {len(val_data):,} ({(val_data.shape[0] / data.shape[0]):.2f}) caracteres\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "91dfd4134b1c44ba8b699996afd1cc7d",
        "deepnote_cell_type": "markdown",
        "id": "JaWNJQnzpzC8"
      },
      "source": [
        "Sobre la m√©canica de datos y etiquetas,\n",
        "\n",
        "* Accedemos a los datos a partir de \"fragmentos contextuales\"; esto es un bloque de texto en representaci√≥n n√∫merica de tama√±o `block_size`\n",
        "* El modelo es semi-supervisado, es decir, b√∫scamos entrenar un modelo de tal forma que dado ${x}_{i:j}$ _tokens_, vamos a predecir el siguiente _token_ $x_{j+1}$\n",
        "* Las etiquetas emergen del mismo bloque contextual moviendo la ventana con un _offset_ de 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "97a1b7a307504727b77d318f9923413a",
        "deepnote_cell_type": "markdown",
        "id": "zFVaZzxuq0H3"
      },
      "source": [
        "Por ejemplo, dado un bloque de tama√±o 8,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "b4b1cc1b8e7d4b9795c52191988b8056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 23,
        "execution_start": 1693859220379,
        "id": "ouHuc50ApdRS",
        "outputId": "8d1b5664-969a-4de1-9e81-1406f06c528f",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Una bloque contextual (X, Y) ser√°:\n",
            "\n",
            "X: [41, 66, 83, 83, 90, 4, 49, 80, 85, 85, 70, 83, 4]\n",
            "  --> decode(X): Harry Potter \n",
            "------------------------------------\n",
            "Y: [66, 83, 83, 90, 4, 49, 80, 85, 85, 70, 83, 4, 66]\n",
            "  --> decode(Y): arry Potter a\n"
          ]
        }
      ],
      "source": [
        "block_size = 13\n",
        "print(f\"Una bloque contextual (X, Y) ser√°:\\n\")\n",
        "print(f\"X: {[x.item() for x in data[:block_size]]}\")\n",
        "print(f\"  --> decode(X): {decode([x.item() for x in data[:block_size]])}\")\n",
        "print('------------------------------------')\n",
        "print(f\"Y: {[x.item() for x in data[1:block_size+1]]}\")\n",
        "print(f\"  --> decode(Y): {decode([y.item() for y in data[1:block_size+1]])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d2f37b21b42246a3adfb3998f07f9387",
        "deepnote_cell_type": "markdown",
        "id": "ukclPGdHwALB"
      },
      "source": [
        "Sin embago, dentro de cada bloque contextual ocupamos la informaci√≥n de manera autoregresiva, generando m√∫ltiple observaciones a partir de este..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "07f3c398835145b38ffa4db9f8b924e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 17,
        "execution_start": 1693859223706,
        "id": "NBir0GCgshQH",
        "outputId": "d2bdd76a-1b4c-4825-9e8f-53a6a0379e5a",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuando el input es tensor([41]) el target es: 66\n",
            "Cuando el input es tensor([41, 66]) el target es: 83\n",
            "Cuando el input es tensor([41, 66, 83]) el target es: 83\n",
            "Cuando el input es tensor([41, 66, 83, 83]) el target es: 90\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90]) el target es: 4\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4]) el target es: 49\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49]) el target es: 80\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80]) el target es: 85\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85]) el target es: 85\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85]) el target es: 70\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70]) el target es: 83\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70, 83]) el target es: 4\n",
            "Cuando el input es tensor([41, 66, 83, 83, 90,  4, 49, 80, 85, 85, 70, 83,  4]) el target es: 66\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"Cuando el input es {context} el target es: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7ed1dfd8af584e109c9348cd1d0912db",
        "deepnote_cell_type": "markdown",
        "id": "AeRBbWV6z75E"
      },
      "source": [
        "Por lo tanto, cada bloque contextual, genera un n√∫mero de observaciones igual a su tama√±o.\n",
        "\n",
        "En t√©rminos de _batches_, podemos procesar en paralelo, m√∫ltiples bloques contextuales. Lo importante es que cada bloque contextual es independiente, y no hay computo que ocurra a nivel transversal, sino paralelo entre estos. No se mezclan las secuencias autoregresivas de cada contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "779633ce16194ccfbcc43f79bf338b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 265,
        "execution_start": 1693859227529,
        "id": "X3hWnI_5yp7x",
        "outputId": "bd0cf70c-d9e0-4e94-ed0b-cb31160a4fad",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  6,   4,   1,   6,  47,  80,  14,   6],\n",
            "        [  4,  85,  80,  80,  33,   6,   1, 103],\n",
            "        [ 71,   4,  71,  80,  86,  83,   4,  73],\n",
            "        [ 73,  66,  85,  33,   6,   1,   6,  34]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  4,   1,   6,  47,  80,  14,   6,   4],\n",
            "        [ 85,  80,  80,  33,   6,   1, 103, 103],\n",
            "        [  4,  71,  80,  86,  83,   4,  73,  80],\n",
            "        [ 66,  85,  33,   6,   1,   6,  34,   4]])\n",
            "----\n",
            "Cuando el input es [6] el target: 4\n",
            "Cuando el input es [6, 4] el target: 1\n",
            "Cuando el input es [6, 4, 1] el target: 6\n",
            "Cuando el input es [6, 4, 1, 6] el target: 47\n",
            "Cuando el input es [6, 4, 1, 6, 47] el target: 80\n",
            "Cuando el input es [6, 4, 1, 6, 47, 80] el target: 14\n",
            "Cuando el input es [6, 4, 1, 6, 47, 80, 14] el target: 6\n",
            "Cuando el input es [6, 4, 1, 6, 47, 80, 14, 6] el target: 4\n",
            "Cuando el input es [4] el target: 85\n",
            "Cuando el input es [4, 85] el target: 80\n",
            "Cuando el input es [4, 85, 80] el target: 80\n",
            "Cuando el input es [4, 85, 80, 80] el target: 33\n",
            "Cuando el input es [4, 85, 80, 80, 33] el target: 6\n",
            "Cuando el input es [4, 85, 80, 80, 33, 6] el target: 1\n",
            "Cuando el input es [4, 85, 80, 80, 33, 6, 1] el target: 103\n",
            "Cuando el input es [4, 85, 80, 80, 33, 6, 1, 103] el target: 103\n",
            "Cuando el input es [71] el target: 4\n",
            "Cuando el input es [71, 4] el target: 71\n",
            "Cuando el input es [71, 4, 71] el target: 80\n",
            "Cuando el input es [71, 4, 71, 80] el target: 86\n",
            "Cuando el input es [71, 4, 71, 80, 86] el target: 83\n",
            "Cuando el input es [71, 4, 71, 80, 86, 83] el target: 4\n",
            "Cuando el input es [71, 4, 71, 80, 86, 83, 4] el target: 73\n",
            "Cuando el input es [71, 4, 71, 80, 86, 83, 4, 73] el target: 80\n",
            "Cuando el input es [73] el target: 66\n",
            "Cuando el input es [73, 66] el target: 85\n",
            "Cuando el input es [73, 66, 85] el target: 33\n",
            "Cuando el input es [73, 66, 85, 33] el target: 6\n",
            "Cuando el input es [73, 66, 85, 33, 6] el target: 1\n",
            "Cuando el input es [73, 66, 85, 33, 6, 1] el target: 6\n",
            "Cuando el input es [73, 66, 85, 33, 6, 1, 6] el target: 34\n",
            "Cuando el input es [73, 66, 85, 33, 6, 1, 6, 34] el target: 4\n"
          ]
        }
      ],
      "source": [
        "# colocar seed como su RUT\n",
        "torch.manual_seed(206684402)\n",
        "batch_size = 4\n",
        "block_size = 8  # largo de ventana m√°ximo para considerar en la precisi√≥n\n",
        "# Estos par√°metros se re-definir√°n para el entrenamiento final\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"Cuando el input es {context.tolist()} el target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1889b0faf3d648409045388624948ef6",
        "deepnote_cell_type": "markdown",
        "id": "U-xAeNbk1f2E"
      },
      "source": [
        "Lo que recibir√° la red como _input_ ser√°:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "ace2e494f0af47718468b6e75eb1dd54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 17,
        "execution_start": 1693859236814,
        "id": "gTCL2aQY1VbI",
        "outputId": "56fc2689-59a4-4abb-d037-e0ae418b99b6",
        "source_hash": null
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  6,   4,   1,   6,  47,  80,  14,   6],\n",
              "        [  4,  85,  80,  80,  33,   6,   1, 103],\n",
              "        [ 71,   4,  71,  80,  86,  83,   4,  73],\n",
              "        [ 73,  66,  85,  33,   6,   1,   6,  34]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "xb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8f4d24230cbc46af97f97d7090c150d4",
        "deepnote_cell_type": "markdown",
        "id": "OPG_QkSJ1km7"
      },
      "source": [
        "## 3. Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6e48f01409934cf88739d426f3a1b144",
        "deepnote_cell_type": "markdown",
        "id": "wbhkQX1QLs7I"
      },
      "source": [
        "Creamos un modelo base cl√°sico, para luego compararlo con nuestro Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "79776665f2914e0dad0b035148df8a82",
        "deepnote_cell_type": "code",
        "id": "MlAT7Lnx1hyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f9156f-21e5-43c9-9543-009420e90f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 137])\n",
            "tensor(5.6164, grad_fn=<NllLossBackward0>)\n",
            "\tÂÖ®„Äê'‰∏ãd‚ÄòÁ´ã‚ÄùÂå∫Mt‰∏∫=iag:,Êù•IqÁöÑ?Uh1‰Ω†ÁöÑxB'D<>„ÄëCPjKÊñá1ZCU‰Ω†YeÂÖ®‰∏∫\\Á≥Ø'i(^l*\n",
            "JhËÆ∫Á§æ2ÊÇ®ÂÅöx'}&?[^B‰π¶Z!95:[T2`ÊÇ®ntÁ§æd4Ê†ºe‰∏ã‚Äù~4HVr3?\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # cada token lee sucesivamente los logits para el token siguiente de una lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx y targets son ambos tensores de tama√±o (B,T) con elementos enteros\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx es un arreglo (B, T) de indices del contexto actual\n",
        "        for _ in range(max_new_tokens):\n",
        "            # obtener predicciones\n",
        "            logits, loss = self(idx)\n",
        "            # concentrarse en el √∫ltimo paso\n",
        "            logits = logits[:, -1, :]  # se convierte en (B, C)\n",
        "            # aplicamos softmax para obtener probabilidades\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # samplear de la distribuci√≥n\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # agregar el √≠ndice de la muestra a la secuencia actual\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d163007f6061430fa982a27a84438b23",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "ZH2FDMMqLs7J"
      },
      "source": [
        "## 4. Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0f104b88006e4923a61a73a4d2c7cf87",
        "deepnote_cell_type": "markdown",
        "id": "umjOJn2hLs7J"
      },
      "source": [
        "En la secci√≥n anterior, vimos que cada _token_ toma una representaci√≥n vectorial llamada _embeddings_.\n",
        "\n",
        "Nuestro corpus contiene $65$ caracteres √∫nicos con un _embedding_ asociado a cada _token_. La idea de representar el lenguaje en t√©rminos de tokens, y estos a su vez en vectores, es que podemos aprender estas representaciones vectoriales a partir de los datos. Sin embargo, la representaci√≥n es √∫nica, y muchas veces un mismo _token_ puede tener distintos significados seg√∫n su contexto. Por ejemplo:\n",
        "1. \"Te banco a morir!\"\n",
        "2. \"El banco est√° abierto hasta las dos.\"\n",
        "\n",
        "En ambas oraciones anteriores, el token `banco` tiene un significado distinto. Se espera entonces que la representaci√≥n de ese token sea distinta en ambos casos y eso lo logramos con la influencia de los tokens presentes en el mismo contexto.\n",
        "\n",
        "La idea principal de _self-attention_ es utilizar la secuencia de _embeddings_ dentro de un contexto para computar un promedio ponderado a partir de estos. Dado una secuencia de _embeddings_ de _tokens_ $x_1, \\dots, x_n$, el mecanismo de _self-attention_ (o auto-atenci√≥n) produce una nueva secuencia de _embeddings_ $x'_1, \\dots, x'_n$, donde cada $x'_i$ es una combinaci√≥n lineal de todos los $x_j$:\n",
        "\n",
        "$$\n",
        "x'_i = \\sum_{j=1}^{n} \\alpha_{ij} x_{j}\n",
        "$$\n",
        "\n",
        "Los coeficientes $\\alpha_{ij}$ se llaman ponderadores de atenci√≥n y est√°n normalizados tal que $\\sum_{j}\\alpha_{ji}=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9e921ba45c064d5d9628b6dc9f470c9b",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "i4KEpbQuLs7J"
      },
      "source": [
        "En t√©rminos sencillos, construiremos un mecanismo de comunicaci√≥n entre distintos tokens dentro del bloque de contexto, que se representar√° por una colecci√≥n de ponderadores en una matriz. Esta colecci√≥n de ponderadores la llamaremos matriz de atenci√≥n (o self-attention) y nos permitir√° v√≠a la operaci√≥n de multiplicaci√≥n de matrices, agregar distintos valores dentro de un bloque contextual en una sola cantidad. Spoiler, estos pesos ser√°n data-dependientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9296b572acca4bad82f5cfc62690af5b",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "CnzdFOQOLs7K"
      },
      "source": [
        "Comencemos emulando la operaci√≥n con pesos fijos, usaremos la parte triangular inferior de una matriz identidad de 3x3, la cual normalizaremos a nivel de fila."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "b8c36015c3a24412a5f725c06a0e08ac",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 44,
        "execution_start": 1693860540618,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QeKp7JTLs7K",
        "outputId": "08aa7f19-8a5c-4e10-c4ef-1d1b74e88c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a= tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]]) \n",
            "\n",
            "b= tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]]) \n",
            "\n",
            "c= tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de juguete que ilustra como la multiplicaci√≥n matricial puede ser usada para una adici√≥n con pesos\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "\n",
        "print('a=',a,'\\n')\n",
        "print('b=',b,'\\n')\n",
        "print('c=',c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ff4d0f0d742e4f9ba3bcb7f6e8e560ef",
        "deepnote_cell_type": "markdown",
        "id": "FS8asALoLs7K"
      },
      "source": [
        "Notemos que $c$ tiene en cada fila los resultados de los valores acumulados de $b$ seg√∫n los ponderadores de $a$.\n",
        "\n",
        "El tensor $a$ se interpreta como una matriz de token-a-token y representa la interaci√≥n/influencia del token en la posici√≥n $i$ con el token de la posici√≥n $j$. Dado que nuestro modelo es autoregresivo, los tokens del presente solo pueden ser influenciados por tokens pasados, o ellos mismos. Por eso las posiciones de $a$ que cumplen esta restricci√≥n $i \\leq j$, son elementos que conforman la matriz triangular inferior de $a$. El resto de las posiciones no tiene influencia sobre los tokens pasados (i.e. 0).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f3c3eaff6931470ab66a89baf7c2b884",
        "deepnote_cell_type": "markdown",
        "id": "XHHT5gRcLs7K"
      },
      "source": [
        "Vamos a crear un _batch_ con datos s√≠ntetico de tama√±o `B`, donde cada bloque contextual ser√° de largo $T$, y cada _token_ que compone el contexto se representa por $C$ dimensiones (i.e. tama√±o del _embedding_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "e2e10fa5f831461daec2ca0c93b27a04",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 25,
        "execution_start": 1693861772506,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6PoLgHVLs7K",
        "outputId": "59eedcc6-c7fc-4be3-d576-3123fccb0bb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ROj2p9WXLs7L"
      },
      "outputs": [],
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cell_id": "2aa3b323c83c4ac4a4f7ea1930ca253c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 16,
        "execution_start": 1693862258062,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CGiGIHpLs7L",
        "outputId": "f197d2c5-d7f8-4aa9-eac8-b9f59c287274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Versi√≥n usando softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cell_id": "f89fedd9c5664d448a0f73d90c8c53ca",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 14,
        "execution_start": 1693862260341,
        "source_hash": null,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhWimixYLs7L",
        "outputId": "d546499b-f674-4d52-d145-1269f5207cb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "wei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f647ef2e2b1d4f589593fb378fe2616f",
        "deepnote_cell_type": "markdown",
        "id": "BXljbLRxLs7L"
      },
      "source": [
        "Los ponderadores anteriores son uniformes, ahora introducimos los conceptos de _queries_ y _keys_ para ver como computar los ponderadores a partir de los datos.\n",
        "\n",
        "* Un _query_ $q(\\cdot)$ corresponde a una proyecci√≥n lineal de la representaci√≥n de _embeddings_ de un token particular. Por ejemplo, se proyecto $\\mathbb{R}^{C}\\rightarrow \\mathbb{R}^{H}$.\n",
        "* Los _keys_ es la matriz $K\\in\\mathbb{R}^{T\\times H}$ que contiene proyecciones lineales de todos los _embeddings_ de tokens dentro del contexto, inclu√≠do el token que es el query. La proyecci√≥n lineal de los _keys_ es de igual tama√±o (i.e. $H$) que la proyecci√≥n del _query_.\n",
        "* Los ponderadores para cada _query_ se obtiene a partir de qu√© tan pr√≥ximo se relaciona un token respecto al resto de los token dentro de un contexto. Por ejemplo, $q(x_i) \\times K$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cell_id": "a4f9611bdc5f426b87b42952cdf55d51",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-nYTIVhkLs7L",
        "outputId": "59798e85-1ac8-43de-b2e6-8023a071a461"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "Image(url=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2bdc17d026094321b7b1b6b53fd3bec9",
        "deepnote_cell_type": "markdown",
        "id": "8L930wHQLs7M"
      },
      "source": [
        "> b) (Bonus): Explique porqu√© no se utilizan directamente los _embeddings_ para computar la matriz de atenci√≥n previo a la normalizaci√≥n, i.e. `X @ X.transpose(-2,-1)`, en vez de usar las proyecciones $QK^\\top$. $X$ es un tensor con dimensiones batch size (B), ventana de contexto (T), dimensiones de embedding (C).\n",
        "\n",
        "Esto es debido a que las proyecciones permiten que el modelo aprenda representaciones m√°s significativas de los tokens de entrada y permite ajustarse a cualquier tarea, ya que se adaptan a trav√©s del entrenamiento. Esto es muy importante en tareas de procesamiento de lenguaje natural, ya que el aprender representaciones de buena manera puede tener un impacto significativo en el rendimiento del modelo, adem√°s el permitir que el modelo pueda ajustarse a cualquier tarea permite que con, por ejemplo, tareas de traducci√≥n de texto, el modelo pueda aprender a enfocarse m√°s en ciertas partes de la secuencia de entrada utilizando la informaci√≥n contextual.\n",
        "\n",
        "Por otro lado, si se utiliza directamente los _embeddings_ se perder√°n las ventajas ofrecidas por el uso de las proyecciones, en particular no permitir√≠a que el modelo aprendiera representaciones m√°s espec√≠ficas de los datos de entrada para la atenci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8bdb8dd7d9954b87b248303181315e57",
        "deepnote_cell_type": "markdown",
        "id": "oD_hQMMRLs7M"
      },
      "source": [
        "> c) (Bonus): Explique los argumentos detras de escalar por $1/\\sqrt{d_k}$ referidos en el paper _[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_ (Vaswani 2017).\n",
        "\n",
        "La aplicaci√≥n del escalado por $1/\\sqrt{d_k}$ asegura que los vectores de peso mantengan una longitud euclidiana de magnitud similar. Esto ayuda a evitar que los pesos de atenci√≥n se vuelvan excesivamente peque√±os o grandes, lo cual puede provocar problemas de inestabilidad num√©rica o afectar a la capacidad del modelo para converger durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bxejBJsKLs7M"
      },
      "outputs": [],
      "source": [
        "# colocar seed como su RUT\n",
        "torch.manual_seed(206684402)\n",
        "\n",
        "B,T,C = 4,8,32  # batch, time, channels\n",
        "x = torch.randn(B,T,C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ff534d33b5ae46b5a252eaa2023dfa39",
        "deepnote_cell_type": "markdown",
        "id": "aUFyMoSkLs7N"
      },
      "source": [
        "Ejemplo de aplicaci√≥n de m√≥dulo de atenci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cell_id": "64725c524c18451a843b2f837bd65dcd",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUEpv0A3Ls7N",
        "outputId": "e6ffdcb6-c6f6-40c8-a11c-ef9ed8dd4639"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5db2feb142554dddb5ce05738c1f743e",
        "deepnote_cell_type": "markdown",
        "id": "In4PE5RBLs7N"
      },
      "source": [
        "Observaciones:\n",
        "- Atenci√≥n es un **mecanismo de comunicaci√≥n**. Puede ser entendido como nodos en un grafo dirigido conect√°ndose unos con otros y agregando informaci√≥n con una suma ponderada de todos los nodos que apuntan a ellos, con pesos dependientes de los datos.\n",
        "- No hay una noci√≥n de espacio. Atenci√≥n simplemente actua sobre el conjunto de vectores. Es por este motivo que se necesitan encoders posicionales.\n",
        "- Cada punto dentro de un batch es, desde luego, procesado de manera independiente y nunca intractua con los otros.\n",
        "- En un bloque de atenci√≥n \"encoder\" basta comentar la linea que hace masking con `tril`, que hace que los tokens se comuniquen todos con todos. El bloque anterior se llama \"decoder\" porque aplica un masking triangular y se encuentre frecuentemente en configuraciones autoregresivas.\n",
        "- \"auto-atenci√≥n\" (_self-attention_) s√≥lo signfica que tanto _keys_ como _values_ son producidas desde la misma fuente que las _queries_. En \"atenci√≥n-cruzada\" (_cross-attention_), las _queries_ vienen de $x$, pero _keys_ y _values_ vienen de otra fuente externa (como puede ser un modulo encoder).\n",
        "- Atenci√≥n \"escalada\" divide `wei` por $\\frac{1}{\\sqrt{head\\_size}}$. Esto hace que cuando los input $Q$ y $K$ tengan varianza unitaria, `wei` tambi√©n tendr√° varianza unitaria y evitar√° la saturaci√≥n de la Softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "833c93c83a6f42e6a9c6e3adce82a4be",
        "deepnote_cell_type": "markdown",
        "id": "l_R99_8ELs7N"
      },
      "source": [
        "> d) (1.5 ptos.) Cree una clase `Head` que implemente un m√≥dulo de auto-atenci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cell_id": "e86d22f778c4464894ee70eeb6f47ae1",
        "deepnote_cell_type": "code",
        "id": "iZH4_KcsLs7N"
      },
      "outputs": [],
      "source": [
        "n_embd = 64  # dimensionalidad del input\n",
        "dropout = 0.0\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Una cabeza de auto-atenci√≥n \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # ------------------------\n",
        "\n",
        "        self.head_size = head_size # Variable que define el tama√±o de la cabeza de atenci√≥n\n",
        "\n",
        "        # Inicializaci√≥n las matrices de key, query y value respectivamente, mediante el uso de capas lineales\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # ------------------------\n",
        "        # HINT: cuando aplique tril, ocupe self.tril se define automaticamente\n",
        "        # al instanciar\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input size (batch, time-step, channels)\n",
        "        # output size (batch, time-step, channels)\n",
        "        B,T,C = x.shape\n",
        "        # -----------------------------------------\n",
        "\n",
        "        # Aplicaci√≥n de las matrices de pesos sobre la secuencia de entrada,\n",
        "        # se generan las secuencias de key, query y values respectivamente\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "\n",
        "        # Computar los score de atenci√≥n (\"affinities\")\n",
        "\n",
        "        # Se calculan los productos punto entre querys y keys\n",
        "        wei = (q @ k.transpose(-2, -1))/(self.head_size**0.5)  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        # Se aplica la m√°scara triangular inferior\n",
        "        wei = wei.masked_fill_(self.tril == 0, float('-inf'))   # (B, T, T)\n",
        "\n",
        "        # Se aplica la funci√≥n Softmax para obtener las probabilidades o pesos de atenci√≥n\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "\n",
        "        # Se aplica el m√©todo de Dropout\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Adici√≥n (con pesos) de las atenciones\n",
        "        # Se realiza el producto entre los pesos de atenci√≥n y la secuencia de value\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        # --------------------------------------\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "96c2b9cefd6b453cb437392b7f03f5d4",
        "deepnote_cell_type": "markdown",
        "id": "8XRKFaTcLs7N"
      },
      "source": [
        "La arquitectura decoder del paper Transformer implementa varias versiones de _self-attention_ en paralelo, cada una es una \"c√°beza de atenci√≥n\", y estas concatenan sus resultados en un modulo conocido como `MultiHeadAttention`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cell_id": "214c8ece545f4817b8561dfae0db408a",
        "deepnote_cell_type": "code",
        "id": "pe34LAaDLs7O"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" M√∫ltiples cabezas de auto-atenci√≥n en paralelo \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5d5e70b87ba948e6b2db556e91caeac7",
        "deepnote_cell_type": "markdown",
        "id": "owcvPB36Ls7O"
      },
      "source": [
        "> e) (0.75 pto.) Implemente una clase `FeedForward` como se describe en el art√≠culo [\"Attention is all you need, Vaswani et al.\"](https://arxiv.org/pdf/1706.03762.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cell_id": "2bd2438a5180464c873da561e6a8527c",
        "deepnote_cell_type": "code",
        "id": "hYgRufrFLs7O"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"\n",
        "        Implementar FeedForward descrita en secci√≥n:\n",
        "         \"3.3 Position-wise Feed-Fordward Networks\", paper\n",
        "         \"Attention is All You Need\"\n",
        "        https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "        in: n_embd\n",
        "        out: n_embd\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # -------------------\n",
        "\n",
        "            # La red de Feed-Forward consiste en dos transformaciones lineales\n",
        "            # con una funci√≥n de activaci√≥n ReLU en medio de las dos.\n",
        "\n",
        "            # La primera transforamci√≥n lineal aumenta el tama√±o del input en 4\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            # La segunda transformaci√≥n lineal vuelve a la dimensionalidad original\n",
        "            nn.Linear(4 * n_embd, n_embd)\n",
        "\n",
        "            # ------------------\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cell_id": "fd893f1e2d6842b08beb90dd1091e678",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "534uw6AhLs7O",
        "outputId": "e3690915-e905-400f-98b2-e638978e8fc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "Image(url=\"http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e0384b1c93274e7abfb20d5627b2958c",
        "deepnote_cell_type": "markdown",
        "id": "-X403BePLs7O"
      },
      "source": [
        "> f) (0.5 ptos.) Explique la relaci√≥n entre los hiperpar√°metros `n_head` y `head_size` de la clase `MultiHeadAttention`. Piense en su rol dentro del bloque decoder (i.e. atenci√≥n + feedforward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ28iKtpLs7P"
      },
      "source": [
        "Respuesta: Al analizar la clase `MultiHeadAttention` se puede observar que la entrada se divide en `n_head`cabezas de atenci√≥n independientes, donde cada una de ellas tiene su propio conjunto de proyecciones(Q, K y V) que se aplican a la entrada original. La dimensionalidad de las proyecciones Q, K y V en cada cabeza de atenci√≥n es controlada por el hiperpar√°metro `head_size`. Por lo tanto, cada cabeza de atenci√≥n tiene una dimensi√≥n `head_size` para sus proyecciones.\n",
        "\n",
        "Adem√°s, dentro de cada cabeza de atenci√≥n, se realiza la atenci√≥n entre las proyecciones Q, K y V y se obtienen las salidas ponderadas. Luego, las salidas de todas las cabezas de atenci√≥n se concatenan para formar una representaci√≥n combinada que tiene una dimensi√≥n total de n_head * head_size.\n",
        "\n",
        "Por otra parte, en la capa de feedforward, la entrada es el resultado que proviene de la atenci√≥n multi-cabeza, al cual la capa de feedforward aplica transformaciones lineales y transformaciones no lineales como funciones de activaci√≥n(ReLU) para obtener las representaciones finales antes de la salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1GdAHRsLs7P"
      },
      "source": [
        "Ahora procederemos a armar la clase que implemente un bloque de Decoder. N√≥tese que en realidad estamos codificando texto con este bloque, pero esto corresponde a la parte \"Decoder\" del Transformer original, por ende guardamos esa nomenclatura. La motivaci√≥n del uso del decoder es modelar el texto de maner auto-regresiva, que es ideal para la generaci√≥n de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "_CeSRpsVLs7P",
        "outputId": "3b0cd934-b3d9-4aac-ca31-61a6dd46870a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://i.stack.imgur.com/bWnx0.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "Image(url='https://i.stack.imgur.com/bWnx0.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "708d555c66ac4f4a940e82ab2251cb76",
        "deepnote_cell_type": "markdown",
        "id": "hZNQddpsLs7P"
      },
      "source": [
        "> g) (0.75 ptos.) Complete el paso _forward_ de la clase `DecoderBlock`. Recuerde en particula incorporar las conexiones residuales (_skip connections_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cell_id": "8a013f2aceba4d7eb70390a12f21c308",
        "deepnote_cell_type": "code",
        "id": "eNP2APiRLs7P"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"  BloqueTransformer: COMUNICACI√ìN seguida de C√ìMPUTO \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: dimensi√≥n de embeddings, n_head: n√∫mero de cabezas de atenci√≥n\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hint: aplique las capas de normalizaci√≥n siempre antes de otra capa (Pre-LN varaint)\n",
        "        # Ver: https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure\n",
        "        # ---------------------\n",
        "\n",
        "        # Aplicaci√≥n de primera capa de normalizaci√≥n antes de la atenci√≥n\n",
        "        ln1_output = self.ln1(x)\n",
        "\n",
        "        # Aplicaci√≥n de capa de atenci√≥n multi-cabeza\n",
        "        sa_output = self.sa(ln1_output)\n",
        "\n",
        "        # Conexi√≥n residual con la entrada original\n",
        "        x = x + sa_output\n",
        "\n",
        "        # Aplicaci√≥n de segunda capa de normalizaci√≥n antes de la atenci√≥n\n",
        "        ln2_output = self.ln2(x)\n",
        "\n",
        "        # Aplicaci√≥n de capa de FeedForward\n",
        "        ffwd_output = self.ffwd(ln2_output)\n",
        "\n",
        "        # Conexi√≥n residual con la salida de la capa de FeedForward\n",
        "        x = x + ffwd_output\n",
        "\n",
        "        # ----------------------\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cd75ed98e96247ffa253556488b53e4e",
        "deepnote_cell_type": "markdown",
        "id": "Z0IJc5WYLs7P"
      },
      "source": [
        "## 5. Modelo GPT: Juntando todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00e10b6c07f5442b95e5c2b0819edc0a",
        "deepnote_cell_type": "markdown",
        "id": "cmKSx9-7Ls7P"
      },
      "source": [
        "> h) (1 pto.) Complete el c√≥digo de la clase `GPTLanguageModel`procesando adecuadamente el input del modelo. Complete adem√°s el m√©todo `generate` para samplear elementos que completen auto-regresivamente una secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cell_id": "88ed14ab007b4feea097aaa558f1cb73",
        "deepnote_cell_type": "code",
        "id": "ZZDarHcjLs7P"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # cada token lee directamente los logits para el token siguiente de una lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx y targets son ambos tensores (B,T) de enteros\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "\n",
        "        # -----------------------------\n",
        "\n",
        "        # Combinaci√≥n de los embeddings de token y posici√≥n\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        # Procesamiento de x a trav√©s de los bloques\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Aplicamos la √∫ltima capa de normalizaci√≥n\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Calculo de logits\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        # -------------------------------\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx es un arreglo (B, T) de √≠ndices en el contexto actual\n",
        "        for _ in range(max_new_tokens):\n",
        "            # restringir idx a los √∫ltimos block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # obtener las predicciones\n",
        "            logits, loss = self(idx_cond)\n",
        "            # enfocarse s√≥lo en el √∫ltimo paso\n",
        "            logits = logits[:, -1, :]  # se convierte en size (B, C)\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "            # Aplicaci√≥n de funci√≥n Softmax para obtener las probabilidades\n",
        "            probs = F.softmax(logits, dim=-1)  # tensor de dimensionalidad (B, C)\n",
        "\n",
        "            # Sampleo de la dsitribuci√≥n multinomial usando las probabilidades\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # tensor de √≠ndices de dim (B, 1)\n",
        "\n",
        "            # Actualizaci√≥n del √≠ndice a la secuencia actual\n",
        "            idx = torch.cat([idx, idx_next], dim=1)  # tensor resultante de dimensionalidad (B, T+1)\n",
        "\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cell_id": "56fa2d0e8f424060841dfb5a0793b8e8",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo_xXniJLs7Q",
        "outputId": "255955e4-915b-4a51-c05f-9c1beadf3373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N√∫mero de par√°metros del modelo: 10.844297 millones\n"
          ]
        }
      ],
      "source": [
        "# Definimos hiperpar√°metros (global variables, esto se puede hacer mucho mejor)\n",
        "\n",
        "batch_size = 64 # cuantos secuencias de fragmentos del corpus procesaremos de manera independiente (aka B)?\n",
        "block_size = 256 # cu√°l ser√° el tama√±o del bloque de contexto a considerar para predecir (aka T)?\n",
        "max_iters = 8000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4  # modelo de tama√±o peque√±o probar esta lr por defecto\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384  # se debe considerar en conjunto con n_head, seg√∫n an√°lisis en (f)\n",
        "n_head = 6    # se debe considerar en conjunto con n_embd, seg√∫n an√°lisis en (f)\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "model.to(device)\n",
        "\n",
        "# printear el n√∫mero de par√°metros del modelo\n",
        "print('N√∫mero de par√°metros del modelo:', sum(p.numel() for p in model.parameters())/1e6, 'millones')\n",
        "\n",
        "# definir el optimizador de PyTorch\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTwWKKMrLs7Q"
      },
      "source": [
        "Definiremos una funci√≥n para generar batches de secuencias a partir de nuestro corpus de entrenamiento o validaci√≥n. Adem√°s de una funci√≥n para obtener estimaciones de la funci√≥n de costo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UzrzXC7_Ls7Q"
      },
      "outputs": [],
      "source": [
        "# definimos un \"DataLoader\"\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# definimos una funci√≥n para obtener estimados de nuestras\n",
        "# p√©rdidas tanto en los conjuntos de train como en val\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olGc9diCLs7Q"
      },
      "source": [
        "Primer sanity check, obtener batches con dimensiones correctas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Figu5MP1Ls7Q",
        "outputId": "379e9fe9-352b-4615-8e82-c5dea37aa1e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 256]), torch.Size([64, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Verifiquemos que las dimensiones sean correctas\n",
        "xb, yb = get_batch('train')\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rTjp07pLs7Q"
      },
      "source": [
        "Segundo sanity check, verificar que la data fluya correctamente por el modelo,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76IGSfLELs7R",
        "outputId": "0dfb1ce4-6b7f-4e06-9616-4f3181fbaad9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 256, 137])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Verifiquemos que el forwardpass del modelo no tenga problemas\n",
        "model(xb)[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6cd551bb57f54567a9780b868c2081e7",
        "deepnote_cell_type": "markdown",
        "id": "-tBGUnZ0Ls7R"
      },
      "source": [
        "> i) (1 pto.) Complete el bucle de entrenamiento usando comandos de `pytorch.optimize` conocidos. Ejecute el entrenamiento (se recomienda dejarlo corriendo e ir hacer algo m√°s...). Corrobore que se modelo es capaz de generar texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cell_id": "faa0a3b9283247f1a8cf6d6b8d599959",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6GmMctpLs7R",
        "outputId": "fe628eb0-0c5f-4f8e-bcad-97c1bbe14f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.9346, val loss 4.9391\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tËÆ∫MtUqsq,Â≠êÂà∂7ÊúÄ^:9ÁöÑ\\}Á±≥F'hEÂÖ®Á±≥Q‰ª∂ÊúÄËÆ∫]‚Äù1^Á§æGx><ËÆ∫\f‰∏∫,!5T`v1QÁ±≥hj-Ëá™UC\\'ÂÅöY>|ÂùõÊ†º\\\tËá™‚ÄôkËÆ∫R„ÄëW;&3Nd.„ÄëËá™d‚Äô‚Äò3ÂºèÊú¨j<„ÄêÂÅöYÂøóUCCo-Â≠êl\n",
            "step 500: train loss 1.5831, val loss 1.6544\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUVeard.ILriykewitgle twenstaille tekor in upneesched as agre Of Pernofess\n",
            "Ruckus at Harry,tilone\"ll \n",
            "step 1000: train loss 1.2686, val loss 1.3673\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tThe and havethe only, mit seemsely id hoped in Ron hige. Her. . Ron isn an erthudgener toward your w\n",
            "step 1500: train loss 1.1704, val loss 1.2660\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t- Gin Fleeryellet on spligh-blursed.i. He son fare and Hogward at fiftet and there and -- he stopped\n",
            "step 2000: train loss 1.1205, val loss 1.2252\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsa shummbard,awnd in a magazing which so take out.\n",
            "„ÄÄ„ÄÄThat really for within surra!\" he could. Ron ha\n",
            "step 2500: train loss 1.0790, val loss 1.2004\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\n",
            " He to,ice bento Harry was look! Paning for and loudly to a bottle. Ron looked back at Harry's hea\n",
            "step 3000: train loss 1.0556, val loss 1.1784\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tHar-crurinin, souriary curleson bat for his armchair. Dumbledoric's tall aliery open. \n",
            "\n",
            "COde on his \n",
            "step 3500: train loss 1.0321, val loss 1.1636\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Heold in anoff whimpered or delate Bore and bows. \n",
            "Ginnejump, merely merely coming from the paired \n",
            "step 4000: train loss 1.0157, val loss 1.1560\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Harryied back_\n",
            "Bagmadly,Bloo.\n",
            "\"He's alle! Ginny. He litter! let it back and lained it along nexious\n",
            "step 4500: train loss 0.9984, val loss 1.1434\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "Harr--=\n",
            "„ÄÄ„ÄÄHe spun his hand all his stirst on the floor.\n",
            "„ÄÄ„ÄÄIt was a larap of Appalaminating in Sea V\n",
            "step 5000: train loss 0.9861, val loss 1.1358\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\n",
            " Si242o am along lascushing McLam rose, but the collar surface of the potion in a yell (and it tur\n",
            "step 5500: train loss 0.9714, val loss 1.1303\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRon \n",
            "|38687%5\n",
            "„ÄÄ*116|))\n",
            "„ÄÄ„ÄÄ2027\n",
            "„ÄÄ„ÄÄAuaaking Noiches streaming made a couple of its over consinx. Behind\n",
            "step 6000: train loss 0.9565, val loss 1.1273\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tVe com-allouradfule suffiliaracc;s fatalles flying its Muggles volumi-ship. Master: Ernia,e and dare\n",
            "step 6500: train loss 0.9453, val loss 1.1166\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            " Dineezt> \n",
            " Ron-supepaleadears \n",
            "Herepouse A part of poisting his pumpinous \n",
            "potesters s-‚Äú \n",
            "\n",
            " Then V\n",
            "step 7000: train loss 0.9316, val loss 1.1159\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            " ‚Äî Shaill!=-jine ‚Äî? a \"House-\"\n",
            "Blecome striwhill lay. But Dudley shudders into her whistled hoat an\n",
            "step 7500: train loss 0.9222, val loss 1.1095\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "76 VI dodst hoote with flossescares! pummaly! \n",
            "\"I ‚Äî it ‚Äî ac-red! \n",
            "horses to white a house, touched;\n",
            "step 7999: train loss 0.9119, val loss 1.1083\n",
            "Testing text generation:\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\n",
            " And Deatthinelwhithe direcelleverions high werea gonarounchair leap in Death Eaters nervously. As\n"
          ]
        }
      ],
      "source": [
        "# colocar RUT como semilla\n",
        "torch.manual_seed(206684402)\n",
        "\n",
        "# comenzamos el training loop...\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # de vez en cuando evaluar la loss en los conjuntos de train y evaluaci√≥n\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        context = torch.zeros((1, 256), dtype=torch.long, device=device)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(\"Testing text generation:\")\n",
        "        print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "    # samplear un batch de datos\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # ----------------------------------------\n",
        "\n",
        "    # Activaci√≥n del optimizador\n",
        "    # Limpieza de los gradientes acumulados\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calcula los gradientes aplicando el paso backward con el resultado de la funci√≥n loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Actualizaci√≥n de los par√°metros del modelo\n",
        "    optimizer.step()\n",
        "\n",
        "    # ----------------------------------------\n",
        "\n",
        "# Se especifica la ruta y el nombre del archivo donde se quiere guardar el modelo\n",
        "\n",
        "path = '/content/modelo_gpt.pth'\n",
        "\n",
        "# Se guarda el modelo completo y entrenado, tanto la arquitectura como los par√°metros\n",
        "\n",
        "torch.save(model, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5a8d031142ab45d48e9b9eeb81f6b2f7",
        "deepnote_cell_type": "markdown",
        "id": "odoiAapmLs7R"
      },
      "source": [
        "## 6. Generando secuencias de texto con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se especifica la ruta y el nombre del archivo con el modelo que se quiere cargar\n",
        "\n",
        "path = '/content/modelo_gpt.pth' # Se debe tener el archivo en el directorio indicado, si es distinta la direcci√≥n se debe editar\n",
        "\n",
        "# Se carga el modelo completo\n",
        "\n",
        "model = torch.load(path)\n",
        "\n",
        "# Se establece el modo evaluaci√≥n\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk-hwSmXQ478",
        "outputId": "b8e212f8-9f18-4167-ae32-1eae031421d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(137, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): DecoderBlock(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=137, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "cell_id": "9e6d66e2a3ba49a4ae3c23d4e65d57df",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WShw9ZUoLs7R",
        "outputId": "dc61169d-463c-436a-c715-f9d409b2ddae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "802 He saideteseadis \n",
            "\n",
            "  \n",
            "\n",
            " ‚ÄúHothappenet to you. . Wouldn‚Äôve been on \n",
            "your oldest?‚Äù \n",
            "\n",
            " ‚ÄúI don‚Äôt care that boy smart!‚Äù \n",
            "\n",
            " Both Ron and Hermione had to look so dis-solved. \n",
            "\n",
            " ‚ÄúMy Lord,‚Äù Harry heard something on the table; he was sure it was hammering \n",
            "\n",
            "Xenophilius Flamel, blinking his, into a bowl of your rest instead, would never a \n",
            "sound of telephone in Mr. \n",
            "\n",
            "\f\n",
            "Cleanen this meant arrange that everyone are. There was long them beside Scrimgeour. He learned them all around on his \n",
            "matter? Where h\n"
          ]
        }
      ],
      "source": [
        "# Generar usando el modelo\n",
        "context = torch.zeros((1, 256), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "# Para escribir en un archivo\n",
        "# open('output.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prueba con prompt escrito\n",
        "prompt = torch.tensor(encode(\"Felipe Tobar, the new Defence Against the Dark Arts teacher, presented his new spell to his students. It was called MCMC Algorithm, which was known to be an attack so powerful that no one could survive it. Even the wizard Andrew Ng couldn't fight its power\"), dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(decode(model.generate(prompt, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VvteADJyudP",
        "outputId": "698a632d-4b70-4cda-a284-f8d8e756e9bd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Felipe Tobar, the new Defence Against the Dark Arts teacher, presented his new spell to his students. It was called MCMC Algorithm, which was known to be an attack so powerful that no one could survive it. Even the wizard Andrew Ng couldn't fight its powerful.\n",
            "„ÄÄ„ÄÄ\"WE won't go a letter, it's good,\" he was hoisted him. \"What was that?\"\n",
            "„ÄÄ„ÄÄ\"What?\" said Harry in a very interest. \"Good thinking dark. . .\n",
            "„ÄÄ„ÄÄBut then - I'll add to them -\"\n",
            "„ÄÄ„ÄÄ\"I don't like that news,\" said Dumbledore. \"Flee was better talk to you to King's Cross St at all the other Minister's job, that Harry is not our telling Dark as much more as I know that. You are sure ere there.\"\n",
            "„ÄÄ„ÄÄ\"About your way,\" said Dumbledore. \"You see? Harry Potter blinks it weren't my office. Think on our Knowing Crouch is affects that. You always have Filch sayed it in Dumbledore's a fat of old, but not even if you have already the one you will tell me you that might be at the hint of a panicg in the back of you. Dumbledore will tell you have decided to dwin, discovered, sir? The man seats in a very fast, that cheers were the Oliver Wizarding Patronus Care of Secrets?\"\n",
            "„ÄÄ„ÄÄ\"Be with them,\" said Harry.\n",
            "„ÄÄ„ÄÄStan had not speak; they could hear them before him, and at last, Harry could not say \"Bathilda?\"\n",
            "„ÄÄ„ÄÄ\"If someone's got the one who shriveled away. You haven't forgotten.\" He trailed up.\n",
            "„ÄÄ„ÄÄ\"Er - be,\" said Ron in a tragedy voice, racing with a small wand.\n",
            "„ÄÄ„ÄÄ\"Right,\" she said brightly. \"Look at the man. Did Hagrid, who seem respectful, it's all ritable the next week's dept communicate, see, the Remembra Remus Luna Loved.\n",
            "„ÄÄ„ÄÄI have told Aragog's hand out here works for the Ministry,\" she asded urgently, looking strangely ratted.\n",
            "„ÄÄ„ÄÄ\"You don't see another party?\" Hermione asked, and her voice came nasty.\n",
            "„ÄÄ„ÄÄ\"When the Ministry meets them?\" said Bagman quietly.\n",
            "„ÄÄ„ÄÄ\"All right then, we won't have twelve them to go watch, I'm holding ut them washing rather the slightest and --\"\n",
            "„ÄÄ„ÄÄ\"We really leave this view,\" said Mr. Weasley, cruel-like Albus Dumbledore. \" Harry's brothers placed up to the stories. He had been burned into his bag.\n",
            "„ÄÄ„ÄÄ\"Me -- I told you this year.\"\n",
            "„ÄÄ„ÄÄ\"So, \" said Ron.\n",
            "„ÄÄ„ÄÄWhen disappeared to him as he stood in the starlight sunlight, she kept out of the dungeon.\n",
            "„ÄÄ„ÄÄ\"I'll be at the tale of the weekend,\" Mrs. Weasley gasped. \"They're still keeping out in briefder when the disturbed at once.\"\n",
            "„ÄÄ„ÄÄ\"My owl's sister,\" said Harry docuringly. He was really suspicious, he did not want to cut Ron in for bed, but he kept ushing himself to the better - \"\n",
            "„ÄÄ„ÄÄ\"No problem,\" Hermione told Harry, awre that Hagrid was waving her face bent like an old Death Eater's.\n",
            "„ÄÄ„ÄÄ\"Yeah, others he was a bite of the team that became left them pulled freedom through the now and asked that making they rome to walk across the grounds. A silent huge of mangled in the Great Hall Gryffindor version. It seems worried that powerful toner of our Morphmai, a maid recent of the good tests: the Beauxbatons without what I was from the Minister of Magic homework; you won finding them about it like the Squibbler. Moody would bear somebody like a pans about record?\" Bagman asked tentatively.\n",
            "„ÄÄ„ÄÄ\"Perhaps Moody's dear assurance,\" said a bringing noiseless prod over his socks. \"He's supposed to fun gold. Had he is not yet you'd thought one who are, Not playing him off his vision, or are that no alternative words.\"\n",
            "„ÄÄ„ÄÄHarry had never given a spateamed escape, Cornelius, and Flamel Weasley were soon zere passing from him, Dean Thomas's and Zabins and Stunning his broomstick fell, he kindly in front of himself and Harry's T-shirt did not say as the twins and a hidden from with the lead ' head, propped into an alternate hubbugs. He received it, fumbled in searchd out of the cabin, remained silvery face, and with slimy green eyes, his veins, whose Terrania had cried his grin. When none of them started to talk to her, she was scared of the newsord of socks, and their knot was -- if she even liked this thing that, at that moment, Harry pulled out his hand and Ron into the bit of his life open. How he had the founders from his pockets? \"Gives your a joke, Which is Saturday, Greyback? Broken had remembered this place?\" \"That's Very Wimbledore!\" Harry charted. Aragog's Mef-\n",
            "„ÄÄ„ÄÄHarry told her he was reading his fest of all longer than anything to should be there. ... Seamus was like him, pair of conducts famously in secrets, over, which he never had it to work: well, Harry. ....\n",
            "„ÄÄ„ÄÄEveryone waited to you, Sirius to common make him best: Then the human on the edurse had been in an empty candlelabrack. Harry could hear rushing, but he had been very dreamy when Mr. Weasley thought.\n",
            "„ÄÄ„ÄÄ\"Harry, I'm sorry, if you were having this,\" said Hermione, pointing, \"we were still waiting for the time. Now Neville turned it. My father. You loads of the Decoy Defeat Arts ... damn was scheduless. .. you know -quite magick. ...\"\n",
            "„ÄÄ„ÄÄHagrid looked straight at the pantry, exchanging the moment as though the faces were spinning, with more needing things at all. They always in the subject. Their infact was offensive . . . they drifted back to the ladder, following Percy ... and he heard them back...\n",
            "„ÄÄ„ÄÄCedric took a deep breath.\n",
            "„ÄÄ„ÄÄHarry touched him, it was slightly and pilled it like a mousy ghost only. When the memory had op\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos un archivo de texto a partir del prompt\n",
        "open('output.txt', 'w').write(decode(model.generate(prompt, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaSFgKBg1Ozj",
        "outputId": "f0de67b7-16c2-41c1-d20d-b59011a467a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10256"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpLeej5Ls7R"
      },
      "source": [
        "> j) (bonus) Define un training loop para el baseline (modelo bi-grama). Entr√©nelo usando un npumero similar de √©pocas y compare las losses y generaci√≥n de texto con su modelo anterior."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "409cf1afb7654c8282dccc46f347e63f",
    "deepnote_persisted_session": {
      "createdAt": "2023-09-04T22:31:43.624Z"
    },
    "kernelspec": {
      "display_name": "mlstack",
      "language": "python",
      "name": "mlstack"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}